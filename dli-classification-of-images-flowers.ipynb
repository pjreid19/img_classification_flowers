{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Description of Problem ( the Challenge ) ( from Kaggle )\n## The Challenge\n* It’s difficult to fathom just how vast and diverse our natural world is.\n* There are over 5,000 species of mammals, 10,000 species of birds, 30,000 species of fish – and astonishingly, over 400,000 different types of flowers.\n* In this competition, you’re challenged to build a machine learning model that identifies the type of flowers in a dataset of images (for simplicity, we’re sticking to just over 100 types).\n## TPUs\n* TPUs are powerful hardware accelerators specialized in deep learning tasks. They were developed (and first used) by Google to process large image databases, such as extracting all the text from Street View. This competition is designed for you to give TPUs a try.\n* TPU quotas are available on Kaggle at no cost to users.","metadata":{}},{"cell_type":"markdown","source":"# Source Notebooks ( thanks!! ) -\n* Competition - Learn how to use Tensor Processing Units (TPUs) on Kaggle\n * https://www.kaggle.com/competitions/tpu-getting-started\n* Petals to the Metal - Flower Classification on TPU\n * 975.4s - TPU v3-8 Public Score - 0.90724\n * https://www.kaggle.com/code/shivam2111/petals-to-the-metal-flower-classification-on-tpu\n* A Beginner's TPU Kernel (Single Model, 0.97) \n * https://www.kaggle.com/code/chankhavu/a-beginner-s-tpu-kernel-single-model-0-97","metadata":{}},{"cell_type":"markdown","source":"# Evaluation Basis\n* Submissions are evaluated on **macro F1 score ( higher is better )**.\n * **F1** = 2 * ( precision * recall ) / ( precision + recall ) or Harmonic Mean of Precision and Recall = 1 / ( ( 1/Recall + 1/Precision ) / 2 )\n * **precision** = TruePositive / ( TruePositive + FalsePositive )\n * **recall** = TruePositive / ( TruePositive + FalseNegative )\n * **Cross-Entropy Loss** = \n* 2022.Nov.30 - Leaderboard #1 **0.98509** ( Marek Nurzynski ) #49 **0.09724** ( ~this source notebook )","metadata":{}},{"cell_type":"markdown","source":"# Versions ( most recent at top )\n* 14 - **0.94241** 2583.4s ( ~40m ) - TPU v3-8\n * 512x512 images, warm-up of model\n * Warm-up 10 EPOCH to val_loss: 0.9334 (on SparseCategoricalCrossentropy) val_sparse_categorical_accuracy: 0.8050\n * Epoch 20 ( at 21m 55s )\n   * loss: 0.0096 - sparse_categorical_accuracy: 0.9966 - categorical_accuracy: 0.0210 - f1: 0.0191 - custom_f1: 1.2450\n   * val_loss: 0.2230 - val_sparse_categorical_accuracy: 0.9526 - val_categorical_accuracy: 0.0210 - val_f1: 0.0189 - val_custom_f1: 1.4303\n   * validation: 0.951\n* 5 - first submission **0.92293** ( 45 of 100+ ) 936.3s - TPU v3-8\n * model = DenseNet201 - Trainable params: 18,292,712\n * train = 40 Epochs, early-stopped on epoch 27 val_sparse_categorical_accuracy 0.9313\n* 1 - base notebook, ? 0.90724 975.4s ( = 15+m )","metadata":{}},{"cell_type":"markdown","source":"# To Do\n* more custom TPU structures\n * Detailed guide to custom training with TPUs\n   * https://www.kaggle.com/code/yihdarshieh/detailed-guide-to-custom-training-with-tpus/notebook\n* Handle over-fitting ( training becomes perfect but validation stays at some level )\n * add dropout\n * augment the source images more\n * train more on the edge conditions\n* Color Analysis\n * given picture, determine the N most common (? representative ) colors\n   * https://stackoverflow.com/questions/3241929/python-find-dominant-most-common-color-in-an-image\n* use psuedo-labeling ( & incremental training ) to resolve the easy ones & focus on the hard ones\n * training should focus efforts on the more-esoteric after easy are done\n * unknowns should be classified in phases from most-known to least\n* use RAPIDS SVR ( uses TPU )\n* Transform images from RGB to **HSV** or other color space\n * then to color spectrum (histogram) vector ( which are the most common colors ( hues ) )\n* Transform images from RGB (+HSV) via 2D FFT & then to 'frequency distribution' vector & ? then to frequency main\n * sound analog - https://towardsdatascience.com/understanding-audio-data-fourier-transform-fft-spectrogram-and-speech-recognition-a4072d228520\n * lower-frequency towards center\n * orientation of frequency = orientation of 2D FFT\n   * we want to be agnostic so we can integrate/average over circles\n* just use pre-trained model resultant embeddings, don't bother re-training/tuning existing models ( that are large )\n* make stand-alone (convolutional) model that doesn't use pre-trained ( ? from dog & cats notebook )\n* use curve-fit to predict quality of model ( as function of effort = epochs and/or time )\n* visualize layer weights & biases to gain understanding ( ? from dog & cats notebook )\n* implement Weights & Balances ( wandb )\n* compare performance ( speed ) on CPU vs. GPU vs. TPU\n * and how to optimize use, initial runs of TPU only use fraction\n* Data Augmentation via:\n * CutMix - https://arxiv.org/abs/1905.04899\n   * mix 2 images via regions and also the labels\n * MixUp - https://arxiv.org/abs/1710.09412\n   * mix 2 images via blend and also the labels","metadata":{}},{"cell_type":"code","source":"# kaggle API use here?\n# https://github.com/Kaggle/kaggle-api\n# creating Kaggle Dataset using Kaggle API\n#!pip install --upgrade --force-reinstall --no-deps kaggle\n\n# from kaggle.api.kaggle_api_extended import KaggleApi\n# api = KaggleApi()\n# api.authenticate()\n# api.competition_download_file('sentiment-analysis-on-movie-reviews', 'train.tsv.zip', path='./')\n# api.competition_download_file('sentiment-analysis-on-movie-reviews', 'test.tsv.zip', path='./')\n\n# !kaggle competitions list -s health  # Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n\n# kaggle competitions {list, files, download, submit, submissions, leaderboard}\n# kaggle datasets {list, files, download, create, version, init}\n# kaggle kernels {list, init, push, pull, output, status}\n# kaggle config {view, set, unset}\n!kaggle datasets list -m\n# upload a dataset using the following API \n#!kaggle datasets create -p /home/jupyter/commonlit/commonlit-v0/\n","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:52:52.378714Z","iopub.execute_input":"2022-12-05T20:52:52.379017Z","iopub.status.idle":"2022-12-05T20:52:53.750612Z","shell.execute_reply.started":"2022-12-05T20:52:52.378923Z","shell.execute_reply":"2022-12-05T20:52:53.749326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run-Mode - Dev ( highly iterative, just to see if things work ) , Test ( general method but not maximal runtime ), Prod/Run ( prob. not EDA, lots of CPU/Memory )\n# Accelerator ( CPU, GPU or TPU )\n# \nclass CFG:\n    USE_TENSORBOARD = False\n    wandb = False\n    competition = 'Petals to the Metal - Flower Classification on TPU'\n    _wandb_kernel = 'nadeau-multi'\n    model = \"Multi\"\n    DO_EDA = False\n    #FEATURE_SELECT = 'Middle'  # 'min', 'all'\n    DO_KERAS = True\n    #DO_LogisticRegression = False\n    #DO_Catboost = False\n    #DO_LightAutoML = False\n    \nN_THREADS = 4\nRANDOM_STATE = 123\n    \ndef class2dict(f):\n    return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:52:53.753485Z","iopub.execute_input":"2022-12-05T20:52:53.753756Z","iopub.status.idle":"2022-12-05T20:52:53.761412Z","shell.execute_reply.started":"2022-12-05T20:52:53.753722Z","shell.execute_reply":"2022-12-05T20:52:53.760119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing Dependencies","metadata":{}},{"cell_type":"code","source":"# good to have lightweight start cell to show that notebook is running\nfrom datetime import datetime as dt\nimport pytz\ntz_NY = pytz.timezone('America/New_York')\n\nimport time\ntime_start = time.time()\n\nprint( \"Local (NY) Time: \", dt.now( tz_NY ).strftime(\"%Y-%m-%d %H:%M:%S\") )","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:52:53.762858Z","iopub.execute_input":"2022-12-05T20:52:53.763258Z","iopub.status.idle":"2022-12-05T20:52:53.798889Z","shell.execute_reply.started":"2022-12-05T20:52:53.763224Z","shell.execute_reply":"2022-12-05T20:52:53.797935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nprint( 'pd.__version__ =', pd.__version__ ) # pd.__version__ = 1.3.2\npd.set_option('display.max_rows', 300)  # or 1000\npd.set_option('display.max_columns', 500)  # or 1000\npd.set_option('display.width', 500)  # or 1000\npd.set_option('display.max_colwidth', None )  # or 199\n\nimport numpy as np \nimport re\nimport math\n\nimport sklearn\nprint( 'sklearn.__version__ =', sklearn.__version__ )  # v 0.23.2 is back-rev, expect 1.02.+\n#from sklearn.metrics import f1_score, precision_score, recall_score, precision_recall_fscore_support\n\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nprint( \"Tensorflow version \" + tf.__version__ ) # Tensorflow version 2.4.1\nfrom tensorflow_addons.metrics import F1Score\n\n#import kernel_tensorflow_utils as ktu  # for HardwareInfo, ImageTransform, LRSchedulers # exists as kernel_tensorflow_utils.py in Input/Utility Scripts directory ","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:52:53.801854Z","iopub.execute_input":"2022-12-05T20:52:53.802307Z","iopub.status.idle":"2022-12-05T20:52:56.103848Z","shell.execute_reply.started":"2022-12-05T20:52:53.802261Z","shell.execute_reply":"2022-12-05T20:52:56.10269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random \ndef set_seed( x: int=RANDOM_STATE, threads: int=N_THREADS ) -> None: \n    \"\"\"Sets seed for results reproducibility.\"\"\"\n    random.seed(x)\n    np.random.seed(x)\n#     torch.manual_seed(x)\n#     torch.backends.cudnn.deterministic = True\n#     torch.backends.cudnn.benchmark = False\n#     torch.set_num_threads(threads)\n#     if torch.cuda.is_available(): torch.cuda.manual_seed_all(x)\n    tf.random.set_seed(x)\n\nset_seed()","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:52:56.105298Z","iopub.execute_input":"2022-12-05T20:52:56.105743Z","iopub.status.idle":"2022-12-05T20:52:56.113592Z","shell.execute_reply.started":"2022-12-05T20:52:56.105699Z","shell.execute_reply":"2022-12-05T20:52:56.112492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Monitor via Weights & Balances ( wandb )","metadata":{}},{"cell_type":"code","source":"if CFG.wandb :  # CFG.wandb:\n    # wandb\n    !rm -rf ./wandb/\n    !pip install --upgrade -q wandb\n    import wandb\n    print('wandb.__version__', wandb.__version__)\n    from wandb.integration.keras import WandbCallback\n    try:\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n        #api_key = user_secrets.get_secret('wandb_key')\n        #wandb.login( key = api_key )\n        secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n        wandb.login( key = secret_value_0 )\n        anonymous = None\n        # 1. Start a new wandb_run\n        # At the top of your training script, start a new run       \n        wandb_run = wandb.init(\n            settings=wandb.Settings(start_method=\"thread\"), # For versions prior to 0.13.0 we suggest using\n            project='Classification-Flowers',\n            entity = 'tnadeau',\n            name = 'Classification-Multi',\n            notes = 'Notes go here.',\n            config = class2dict(CFG),\n            group = 'GPU',\n            job_type = \"train\",\n            anonymous = anony,\n            reinit = True  # to enable multiple 'runs' from a single script\n        )\n        # Capture a dictionary of hyperparameters with config\n        wandb.config = {\n                \"n_estimators\": 500, # default = 100\n                \"max_depth\": 4, # default = 3\n                \"min_samples_split\": 5,  # default = 2\n                \"learning_rate\": 0.01,  # def = 0.01\n                \"loss\": \"squared_error\", # def = 'squared_error'\n            }\n        \n    except:\n        wandb.login( anonymous = 'must' )\n        print('To use your W&B account,\\nGo to Add-ons -> Secrets and provide your \\\n               W&B access token. Use the Label name as WANDB. \\nGet your W&B access \\\n               token from here: https://wandb.ai/authorize')","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:52:56.115509Z","iopub.execute_input":"2022-12-05T20:52:56.115848Z","iopub.status.idle":"2022-12-05T20:52:56.137155Z","shell.execute_reply.started":"2022-12-05T20:52:56.115811Z","shell.execute_reply":"2022-12-05T20:52:56.135665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup TPU and Distribution strategy","metadata":{}},{"cell_type":"code","source":"!nvidia-smi\n# GPU T4 x2 - NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4 - 2 x Tesla T4\n# GPU P100 - NVIDIA - Tesla P100-PCIE...","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:52:56.139625Z","iopub.execute_input":"2022-12-05T20:52:56.140201Z","iopub.status.idle":"2022-12-05T20:52:57.193422Z","shell.execute_reply.started":"2022-12-05T20:52:56.140143Z","shell.execute_reply":"2022-12-05T20:52:57.192502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect TPU, return appropriate distribution strategy\n\n# for GPU something like:\n# https://www.kaggle.com/code/landlord/numba-cuda-mandelbrot\n# from numba import cuda\n# from number import * \n# new_func = cuda.jit( restype =, argtypes = , device = True)(old_func)\n# @cuda.jit( argtypes = []) def new_func\n# ? cuda.grid(2)\n# cuda.to_device( image )\n\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print( 'Running on TPU .master() =', tpu.master())  # Running on TPU  grpc://10.0.0.2:8470\n    print( 'cluster_spec()', tpu.cluster_spec() )  # ClusterSpec({'worker': ['10.0.0.2:8470']})\n    print( '.get_job_name()', tpu.get_job_name())\n    print( '.get_master()', tpu.get_master())\n    print( '.get_tpu_system_metadata()', tpu.get_tpu_system_metadata())\n    print( '.num_accelerators()', tpu.num_accelerators())\n    print( '.task_id', tpu.task_id)\n    print( '.task_type', tpu.task_type) # worker\nexcept ValueError:\n    tpu = None\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\nnum_gpus = len(gpus)    \n\nif tpu:\n    tf.config.experimental_connect_to_cluster( tpu )\n    tf.tpu.experimental.initialize_tpu_system( tpu )\n    strategy = tf.distribute.experimental.TPUStrategy( tpu )\nelif gpus :\n    print( 'Running on GPU(s)', num_gpus )\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth( device = gpu, enable = True )\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        print(num_gpus, \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n        print(e)\n    if num_gpus == 0:\n        strategy = tf.distribute.OneDeviceStrategy(device='CPU')\n        print(\"Setting strategy to OneDeviceStrategy(device='CPU')\")\n    elif num_gpus == 1:  # Accelerator = GPU P100\n        strategy = tf.distribute.OneDeviceStrategy(device='GPU')\n        print(\"Setting strategy to OneDeviceStrategy(device='GPU')\")\n    else:\n        strategy = tf.distribute.MirroredStrategy()\n        print(\"Setting strategy to MirroredStrategy()\")  # seems to run out of memory\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint( \"REPLICAS: \", strategy.num_replicas_in_sync )\n# REPLICAS:  8 ? TPU 1VM v3-8\n# REPLICAS:  2 GPU T4 x 2\n# REPLICAS:  1 CPU, GPU P100 \n# if CPU ( not GPU/TPU ) probably do not 'augment' images","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:52:57.195637Z","iopub.execute_input":"2022-12-05T20:52:57.195898Z","iopub.status.idle":"2022-12-05T20:53:03.128879Z","shell.execute_reply.started":"2022-12-05T20:52:57.195866Z","shell.execute_reply":"2022-12-05T20:53:03.128112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Mixed Precision and/or XLA¶\nThe following booleans can enable mixed precision and/or XLA on GPU/TPU. By default TPU already uses some mixed precision but we can add more. These allow the GPU/TPU memory to handle larger batch sizes and can speed up the training process. The Nvidia V100 GPU has special Tensor Cores which get utilized when mixed precision is enabled. Unfortunately Kaggle's Nvidia P100 GPU does not have Tensor Cores to receive speed up.","metadata":{}},{"cell_type":"code","source":"MIXED_PRECISION = True\nXLA_ACCELERATE = True\n\nif MIXED_PRECISION:\n    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n    if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n    else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n    mixed_precision.set_policy(policy)\n    print('Mixed precision enabled')\n\nif XLA_ACCELERATE:\n    tf.config.optimizer.set_jit(True)\n    print('Accelerated Linear Algebra enabled')","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:53:03.130771Z","iopub.execute_input":"2022-12-05T20:53:03.131031Z","iopub.status.idle":"2022-12-05T20:53:03.138216Z","shell.execute_reply.started":"2022-12-05T20:53:03.130999Z","shell.execute_reply":"2022-12-05T20:53:03.137182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run-Time Configuration","metadata":{}},{"cell_type":"code","source":"# Helper variables\nAUTO = tf.data.experimental.AUTOTUNE  # tf.data.AUTOTUNE\nIMAGE_SHAPE = [512,512]   #  [192,192], [512,512] # ?this does 'resize', no instead seems to pick a sub-set of images & work from there\nif num_gpus > 1 :\n    BATCH_SIZE = 16\nelse :\n    BATCH_SIZE = 16 * strategy.num_replicas_in_sync # for CPU is only 1 but should be 4, for GPU P100 is 1, for TPU is 8\n\nWARMUP_EPOCHS = 20\nTRAIN_EPOCHS = 30\nRANDOM_SEED = 42","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:53:03.139403Z","iopub.execute_input":"2022-12-05T20:53:03.139621Z","iopub.status.idle":"2022-12-05T20:53:03.148272Z","shell.execute_reply.started":"2022-12-05T20:53:03.139595Z","shell.execute_reply":"2022-12-05T20:53:03.147438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Processing","metadata":{}},{"cell_type":"markdown","source":"I am not quite exerienced with the tfrecord and efficient tpu optimization so I spent a huge amout of time on the cloud local error 😅😅😅😅 Then I googled and realised that TPUs cannot access local data they only use cloud verified data and therefore in next cell i am going to get the same data but from google cloud.\n**am still learning ig**","metadata":{}},{"cell_type":"code","source":"from kaggle_datasets import KaggleDatasets\n# google cloud store path\n# Data dirs\n\n\nPATH = KaggleDatasets().get_gcs_path('tpu-getting-started')\nprint(PATH)\n# gs://kds-7187323d9756797ee580f506f69177097d6bce252b2e21ea5ccb7a01\n\ndata_dir_by_size = { # available image sizes\n    (192, 192): '/tfrecords-jpeg-192x192',\n    (224, 224): '/tfrecords-jpeg-224x224',\n    (331, 331): '/tfrecords-jpeg-331x331',\n    (512, 512): '/tfrecords-jpeg-512x512'\n}\n\nGCS_PATH_SELECT = { # available image sizes\n    192: PATH + '/tfrecords-jpeg-192x192',\n    224: PATH + '/tfrecords-jpeg-224x224',\n    331: PATH + '/tfrecords-jpeg-331x331',\n    512: PATH + '/tfrecords-jpeg-512x512'\n}\nsubdir = data_dir_by_size[ ( IMAGE_SHAPE[0], IMAGE_SHAPE[1] ) ]\n\nGCS_PATH = GCS_PATH_SELECT[IMAGE_SHAPE[0]]\nprint(GCS_PATH)\n# gs://kds-7187323d9756797ee580f506f69177097d6bce252b2e21ea5ccb7a01\n# gs://kds-7187323d9756797ee580f506f69177097d6bce252b2e21ea5ccb7a01/tfrecords-jpeg-512x512","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:53:03.149659Z","iopub.execute_input":"2022-12-05T20:53:03.150451Z","iopub.status.idle":"2022-12-05T20:53:03.538202Z","shell.execute_reply.started":"2022-12-05T20:53:03.150404Z","shell.execute_reply":"2022-12-05T20:53:03.537165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tfrec files contain multiple data types at once ( the image, the label, etc. )\n# tf.io.gfile.glob(   pattern )\nTRAINING_IMAGES = tf.io.gfile.glob( f'{GCS_PATH}/train/*.tfrec' ) # glob() returns a list of files that match the given patterns\nVALID_IMAGES = tf.io.gfile.glob( f'{GCS_PATH}/val/*.tfrec' )\nTEST_IMAGES = tf.io.gfile.glob( f'{GCS_PATH}/test/*.tfrec' )\n\n# Extending the dataset with additional data\n# added dataset from: https://www.kaggle.com/datasets/kirillblinov/tf-flower-photo-tfrec\next_gcs = None\n#ext_gcs = KaggleDatasets().get_gcs_path('tf-flower-photo-tfrec')  # not found  Needs to be in local input directory\n\nif ( ext_gcs ) :\n    imagenet_files = tf.io.gfile.glob(ext_gcs + '/imagenet' + subdir + '/*.tfrec')  # (GCS_PATH_EXT + '/imagenet_no_test/tfrecords-jpeg-{size}x{size}/*.tfrec'.format(size=SIZE))\n    inaturelist_files = tf.io.gfile.glob(ext_gcs + '/inaturalist' + subdir + '/*.tfrec')\n    openimage_files = tf.io.gfile.glob(ext_gcs + '/openimage' + subdir + '/*.tfrec')\n    oxford_files = tf.io.gfile.glob(ext_gcs + '/oxford_102' + subdir + '/*.tfrec')\n    tensorflow_files = tf.io.gfile.glob(ext_gcs + '/tf_flowers' + subdir + '/*.tfrec')\n\n    TRAINING_IMAGES = TRAINING_IMAGES + imagenet_files + inaturelist_files + openimage_files + oxford_files + tensorflow_files","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:53:03.539636Z","iopub.execute_input":"2022-12-05T20:53:03.540414Z","iopub.status.idle":"2022-12-05T20:53:03.669173Z","shell.execute_reply.started":"2022-12-05T20:53:03.540374Z","shell.execute_reply":"2022-12-05T20:53:03.668112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CLASSES = ['pink primrose',    'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',     'wild geranium',     'tiger lily',           'moon orchid',              'bird of paradise', 'monkshood',        'globe thistle',         # 00 - 09\n           'snapdragon',       \"colt's foot\",               'king protea',      'spear thistle', 'yellow iris',       'globe-flower',         'purple coneflower',        'peruvian lily',    'balloon flower',   'giant white arum lily', # 10 - 19\n           'fire lily',        'pincushion flower',         'fritillary',       'red ginger',    'grape hyacinth',    'corn poppy',           'prince of wales feathers', 'stemless gentian', 'artichoke',        'sweet william',         # 20 - 29\n           'carnation',        'garden phlox',              'love in the mist', 'cosmos',        'alpine sea holly',  'ruby-lipped cattleya', 'cape flower',              'great masterwort', 'siam tulip',       'lenten rose',           # 30 - 39\n           'barberton daisy',  'daffodil',                  'sword lily',       'poinsettia',    'bolero deep blue',  'wallflower',           'marigold',                 'buttercup',        'daisy',            'common dandelion',      # 40 - 49\n           'petunia',          'wild pansy',                'primula',          'sunflower',     'lilac hibiscus',    'bishop of llandaff',   'gaura',                    'geranium',         'orange dahlia',    'pink-yellow dahlia',    # 50 - 59\n           'cautleya spicata', 'japanese anemone',          'black-eyed susan', 'silverbush',    'californian poppy', 'osteospermum',         'spring crocus',            'iris',             'windflower',       'tree poppy',            # 60 - 69\n           'gazania',          'azalea',                    'water lily',       'rose',          'thorn apple',       'morning glory',        'passion flower',           'lotus',            'toad lily',        'anthurium',             # 70 - 79\n           'frangipani',       'clematis',                  'hibiscus',         'columbine',     'desert-rose',       'tree mallow',          'magnolia',                 'cyclamen',         'watercress',       'canna lily',            # 80 - 89\n           'hippeastrum',      'bee balm',                  'pink quill',       'foxglove',      'bougainvillea',     'camellia',             'mallow',                   'mexican petunia',  'bromelia',         'blanket flower',        # 90 - 99\n           'trumpet creeper',  'blackberry lily',           'common tulip',     'wild rose']                                                                                                                                               # 100 - 103\n\nprint( len( CLASSES ) ) # 104","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:53:03.673162Z","iopub.execute_input":"2022-12-05T20:53:03.673424Z","iopub.status.idle":"2022-12-05T20:53:03.683464Z","shell.execute_reply.started":"2022-12-05T20:53:03.673396Z","shell.execute_reply":"2022-12-05T20:53:03.682458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_files(filenames):\n    '''Count number of files in the dataset ( actually file system )'''\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(file).group(1)) for file in filenames]\n    return np.sum(n)","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:53:03.684678Z","iopub.execute_input":"2022-12-05T20:53:03.684968Z","iopub.status.idle":"2022-12-05T20:53:03.698733Z","shell.execute_reply.started":"2022-12-05T20:53:03.684937Z","shell.execute_reply":"2022-12-05T20:53:03.697789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_TRAIN_IMG = count_files( TRAINING_IMAGES )\nNUM_VALID_IMG = count_files( VALID_IMAGES )\nNUM_TEST_IMG  = count_files( TEST_IMAGES )\nprint(f'Number of training images : {NUM_TRAIN_IMG} \\nNumber of validation images : {NUM_VALID_IMG} \\nNumber of unlabeled test images : {NUM_TEST_IMG}')\n\n# Number of training images : 12753 ( = ~ 16 * 798 = 12768 )\n# with extended dataset this moves up to 68094 training images\n# Number of validation images : 3712 \n# Number of unlabeled test images : 7382\n\n# with additional tf-flower-photo-tfrec, counts for train increases\n# Number of training images : 68094 # increases > 5-fold, does distribution change?\n# Number of validation images : 3712 \n# Number of unlabeled test images : 7382","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:53:03.700309Z","iopub.execute_input":"2022-12-05T20:53:03.700608Z","iopub.status.idle":"2022-12-05T20:53:03.715488Z","shell.execute_reply.started":"2022-12-05T20:53:03.70057Z","shell.execute_reply":"2022-12-05T20:53:03.714704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# do more EDA\n# what is histogram of Train & Valid\n# are there any errors","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:53:03.716758Z","iopub.execute_input":"2022-12-05T20:53:03.71723Z","iopub.status.idle":"2022-12-05T20:53:03.728002Z","shell.execute_reply.started":"2022-12-05T20:53:03.717188Z","shell.execute_reply":"2022-12-05T20:53:03.726861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Helper Functions","metadata":{}},{"cell_type":"code","source":"def decode_image( img ):\n    '''Load Image From The Dataset ( and convert to TF-ready floating point )'''\n    image = tf.io.decode_jpeg( img, channels = 3 )\n    image = tf.cast( image, tf.float32 ) / 255.0  # tf.float32 doesn't need to be tf.float32 since original data is only int8\n    image = tf.reshape( image, [ *IMAGE_SHAPE, 3 ] )\n    return image\n\ndef onehot( image, label ):\n    return image, tf.one_hot( label, len(CLASSES) )","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:53:03.729601Z","iopub.execute_input":"2022-12-05T20:53:03.729898Z","iopub.status.idle":"2022-12-05T20:53:03.739525Z","shell.execute_reply.started":"2022-12-05T20:53:03.72986Z","shell.execute_reply":"2022-12-05T20:53:03.738413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define tfr-record parsing functions\ndef read_labeled_tfrecord( example ):  # example: A string tensor representing a `tf.train.Example`\n    '''Read Labeled tfrecord'''\n    labeled_struct = {\n        'image': tf.io.FixedLenFeature([],tf.string),\n        'class': tf.io.FixedLenFeature([],tf.int64)\n    }\n    parsed = tf.io.parse_single_example( example, labeled_struct)\n    image = decode_image( parsed['image'] )\n    label = tf.cast( parsed['class'], tf.int32 )  # tf.int32 - only 104 different values so could be int16\n    return image, label\n\ndef read_unlabeled_tfrecord( example ):\n    '''Read unlabeled tfrecord'''\n    unlabeled_struct = {\n        'image': tf.io.FixedLenFeature([],tf.string),\n        'id': tf.io.FixedLenFeature([],tf.string)\n    }\n    parsed = tf.io.parse_single_example( example, unlabeled_struct)\n    image = decode_image( parsed['image'] )\n    idnum = parsed['id']\n    return image, idnum","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:53:03.741191Z","iopub.execute_input":"2022-12-05T20:53:03.741875Z","iopub.status.idle":"2022-12-05T20:53:03.751469Z","shell.execute_reply.started":"2022-12-05T20:53:03.741817Z","shell.execute_reply":"2022-12-05T20:53:03.750462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_dataset( filenames, is_labeled = True, inorder = False ):\n    '''Load the tfrecord as Dataset.\n    is_label(bool) : is the data labeled or unlabeled\n    inorder(bool) : Should the data be inorder or loaded as soon as it arrives'''\n    options = tf.data.Options()\n    if not inorder:\n        options.experimental_deterministic = False\n    dataset = tf.data.TFRecordDataset( filenames, num_parallel_reads = AUTO )\n    dataset = dataset.with_options( options )\n    dataset = dataset.map( read_labeled_tfrecord if is_labeled else read_unlabeled_tfrecord )\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:53:03.753071Z","iopub.execute_input":"2022-12-05T20:53:03.753938Z","iopub.status.idle":"2022-12-05T20:53:03.767787Z","shell.execute_reply.started":"2022-12-05T20:53:03.753896Z","shell.execute_reply":"2022-12-05T20:53:03.76694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_aug( image, label ):\n    '''Image Augummentation'''\n    image = tf.image.random_flip_left_right( image ) \n    return image, label","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:53:03.769096Z","iopub.execute_input":"2022-12-05T20:53:03.769521Z","iopub.status.idle":"2022-12-05T20:53:03.779103Z","shell.execute_reply.started":"2022-12-05T20:53:03.769486Z","shell.execute_reply":"2022-12-05T20:53:03.778095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# order should possibly be:  .cache(), .shuffle(), .repeat(), .map(), .filter(), .batch(), .prefetch()\n# our order is: .map(), .repeat(), .shuffle(), .batch(), [.cache()], .prefetch()\ndef get_train_dataset( do_aug = True, do_repeat = True, do_shuffle = True, do_onehot = False ):\n    '''Load the training dataset'''\n    dataset = load_dataset( TRAINING_IMAGES, is_labeled=True)\n    dataset = dataset.map( data_aug, num_parallel_calls = AUTO )\n    if do_onehot:\n        dataset = dataset.map(onehot, num_parallel_calls=AUTO)\n    if do_aug: dataset = dataset.map( transform, num_parallel_calls = AUTO )\n    if do_repeat: dataset = dataset.repeat()\n    if do_shuffle: dataset = dataset.shuffle( 10 * BATCH_SIZE  )  # 49 shuffle(   buffer_size, seed=None, reshuffle_each_iteration=None, name=None )\n    dataset = dataset.batch( BATCH_SIZE )  # could instead be num_parallel_calls = tf.data.AUTOTUNE\n    # dataset = dataset.cache()\n    dataset = dataset.prefetch( buffer_size = AUTO )\n    return dataset\n\ndef get_train_dataset_preview( ordered = True, do_aug = True, do_repeat = True, do_shuffle = True, do_onehot = False ):\n    '''Load the training dataset'''\n    dataset = load_dataset( TRAINING_IMAGES, is_labeled=True, inorder = ordered )\n    if do_onehot:\n        dataset = dataset.map(onehot, num_parallel_calls=AUTO)\n    #dataset = dataset.map( data_aug, num_parallel_calls = AUTO )\n    #if do_aug: dataset = dataset.map( transform, num_parallel_calls = AUTO )\n    #if do_repeat: dataset = dataset.repeat()\n    #if do_shuffle: dataset = dataset.shuffle( 10 * BATCH_SIZE  )  # 49 shuffle(   buffer_size, seed=None, reshuffle_each_iteration=None, name=None )\n    dataset = dataset.batch( BATCH_SIZE )  # could instead be num_parallel_calls = tf.data.AUTOTUNE\n    dataset = dataset.cache()\n    dataset = dataset.prefetch( buffer_size = AUTO )\n    return dataset\n\n#2022-12-02 14:14:08.461447: W tensorflow/core/kernels/data/cache_dataset_ops.cc:757] The calling iterator did not fully read the dataset being cached.\n# In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded.\n# This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\ndef get_valid_dataset( ordered = False, do_onehot = False ):\n    '''Load the validation dataset'''\n    dataset = load_dataset( VALID_IMAGES, is_labeled=True, inorder=ordered)\n    if do_onehot:\n        dataset = dataset.map(onehot, num_parallel_calls=AUTO)\n    dataset = dataset.batch( BATCH_SIZE )\n    dataset = dataset.cache() # perhaps this should be in front of .batch()\n    dataset = dataset.prefetch( buffer_size = AUTO)\n    return dataset\n\ndef get_test_dataset( ordered = False ):\n    '''Load the test dataset'''\n    dataset = load_dataset( TEST_IMAGES, is_labeled=False, inorder=ordered)\n    dataset = dataset.batch( BATCH_SIZE)\n    # dataset = dataset.cache()\n    dataset = dataset.prefetch( buffer_size = AUTO)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:53:03.780767Z","iopub.execute_input":"2022-12-05T20:53:03.781022Z","iopub.status.idle":"2022-12-05T20:53:03.793802Z","shell.execute_reply.started":"2022-12-05T20:53:03.780994Z","shell.execute_reply":"2022-12-05T20:53:03.793085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow.keras.backend as K\n# alternatively use from keras.preprocessing.image import ImageDataGenerator\n# tf.keras.preprocessing.image_dataset_from_directory(\n# img_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n#     rotation_range=54\n#     , width_shift_range=0.15\n#     , height_shift_range=0.15\n#     , brightness_range=None\n#     , zoom_range=[1.0, 1.25]\n#     , fill_mode='constant'\n#     , horizontal_flip=True\n#     , preprocessing_function=None\n# ) \n# image = img_gen.random_transform(image)\n\n#@staticmethod\ndef get_mat( rotation, shear, height_zoom, width_zoom, height_shift, width_shift ) :\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation / 180.\n    shear = math.pi * shear / 180.\n    \n    # ROTATION MATRIX\n    c1 = tf.math.cos(rotation)\n    s1 = tf.math.sin(rotation)\n    one = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n        \n    # SHEAR MATRIX\n    c2 = tf.math.cos( shear )\n    s2 = tf.math.sin( shear )\n    shear_matrix = tf.reshape( tf.concat( [ one, s2, zero, zero,c2,zero, zero,zero,one],axis = 0 ),[ 3,3 ] )    \n    \n    # ZOOM MATRIX\n    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n    \n    # SHIFT MATRIX\n    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))\n\n\ndef transform( image, label ) :  # why is label passed & returned?\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    DIM = IMAGE_SHAPE[0]\n    XDIM = DIM%2 #fix for size 331\n    \n    rot = 15. * tf.random.normal([1],dtype='float32')\n\n    #shr = 5. * tf.random.normal( [1],dtype='float32' )  # Ted doesn't like large shear. Distorts photos \n    shr = 2. * tf.random.normal( [1],dtype='float32' ) \n\n    h_zoom = 1.0 + tf.random.normal([1],dtype='float32') / 10.\n    w_zoom = 1.0 + tf.random.normal([1],dtype='float32') / 10.\n    h_shift = 16. * tf.random.normal([1],dtype='float32') \n    w_shift = 16. * tf.random.normal([1],dtype='float32') \n  \n    # GET TRANSFORMATION MATRIX\n    m = get_mat( rot, shr, h_zoom, w_zoom, h_shift , w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x = tf.repeat( tf.range( DIM//2, -DIM//2, -1 ), DIM )\n    y = tf.tile( tf.range( -DIM//2, DIM//2 ), [DIM] )\n    z = tf.ones( [ DIM * DIM ], dtype = 'int32' )\n    idx = tf.stack( [ x, y, z ] )\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot( m, tf.cast( idx, dtype = 'float32' ) )\n    idx2 = K.cast( idx2, dtype = 'int32')\n    idx2 = K.clip( idx2, -DIM//2 + XDIM + 1, DIM//2 )\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack( [ DIM//2 - idx2[0,], DIM//2 - 1 + idx2[1,] ] )\n    d = tf.gather_nd( image, tf.transpose( idx3 ) )\n        \n    return tf.reshape( d, [ DIM, DIM, 3 ] ), label","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:53:03.795427Z","iopub.execute_input":"2022-12-05T20:53:03.795804Z","iopub.status.idle":"2022-12-05T20:53:03.816196Z","shell.execute_reply.started":"2022-12-05T20:53:03.795771Z","shell.execute_reply":"2022-12-05T20:53:03.815003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/code/odins0n/jax-flax-tf-data-vision-transformers-tutorial\n# augment by changes in Contrast, Brightness, Saturation, Rotation\ndef aug( data ):\n    \"\"\"\n    Augument the image with one of four \n    different equal likely transformations.\n    Transformations:\n    1. Random Contrast\n    2. Random Brightness\n    3. Random Saturation\n    4. Random Rotation\n    \"\"\"\n    image = data[\"image\"]\n    seed = ( RANDOM_SEED, RANDOM_SEED )\n    \n    # two groups for transformations\n    transformation_selection = tf.random.uniform( [], minval = 0, maxval = 1, dtype = tf.float32 )\n    \n    # probability for sub groups - each transformation has 25% chance of being applied on image\n    prob_1 = tf.random.uniform( [], minval = 0, maxval = 1, dtype = tf.float32 )  \n    prob_2 = tf.random.uniform( [], minval = 0, maxval = 1, dtype = tf.float32 )\n    image = tf.cond( tf.greater( transformation_selection, 0.5 )\n                    , lambda: tf.cond( tf.greater( prob_1, 0.5 ),\n                            lambda: tf.image.stateless_random_contrast( image, 0.1, 0.5, seed = seed ), \n                            lambda: tf.image.stateless_random_brightness( image, max_delta = 0.3, seed = seed ),\n                    )\n                    , lambda: tf.cond( tf.greater( prob_2, 0.5 ),\n                            lambda: tf.image.stateless_random_saturation( image, 0.01, 0.1, seed = seed ),\n                            lambda: tfa.image.rotate( image, tf.random.uniform( (1,), minval = 0.01, maxval = 0.2 ) )                          \n                    )     \n    )\n    data[\"image\"] = image\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:53:03.817248Z","iopub.execute_input":"2022-12-05T20:53:03.817855Z","iopub.status.idle":"2022-12-05T20:53:03.831601Z","shell.execute_reply.started":"2022-12-05T20:53:03.817815Z","shell.execute_reply":"2022-12-05T20:53:03.830739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n############# ImageDataGenerator - random transformation #############\n\n# create an ImageDataGenerator \n# update this based on image augmenation exploration results\nimg_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n    featurewise_center=False\n    , samplewise_center=False\n    , featurewise_std_normalization=False\n    , samplewise_std_normalization=False\n    , zca_whitening=False\n    , zca_epsilon=1e-06\n    , rotation_range=36\n    , width_shift_range=0.15\n    , height_shift_range=0.15\n    , brightness_range=None\n    , shear_range = 0.0\n    , zoom_range=[1.0, 1.25]\n    , channel_shift_range = 0.0\n    , fill_mode='constant'  # 'nearest'\n    , cval = 0.0\n    , horizontal_flip=True\n    , vertical_flip = False\n    , rescale = None\n    , preprocessing_function=None\n    , data_format = None\n    , validation_split = 0.0\n    , dtype = None\n)\n\n# define data augmentation function with random_transform method \n# for dataset.map( ... )\ndef img_gen_random_transform(image, label):\n    # apply random_transform method to single image\n    image = img_gen.random_transform(image)\n    return image, label","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:53:03.834936Z","iopub.execute_input":"2022-12-05T20:53:03.835901Z","iopub.status.idle":"2022-12-05T20:53:03.849259Z","shell.execute_reply.started":"2022-12-05T20:53:03.835835Z","shell.execute_reply":"2022-12-05T20:53:03.848189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# show some 'augmentations' of a single image ( ? each epoch each training image will be augmented once )\nrow = 3; col = 7;\nall_elements = get_train_dataset(do_aug=False).unbatch()\none_element = tf.data.Dataset.from_tensors( next(iter(all_elements)) )\naugmented_element = one_element.repeat().map(transform).batch( row * col )  # ? runs out of memory\n\n#plt.figure( figsize = ( 8 * row, 8 * col ) )\nfor (img,label) in augmented_element:\n    plt.figure(figsize=(30,int(30*row/col)))\n    for j in range(row*col):\n        plt.subplot(row,col,j+1)\n        plt.axis('off')\n        plt.imshow(img[j,])\n    plt.show()\n    break\ndel augmented_element","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:53:03.851134Z","iopub.execute_input":"2022-12-05T20:53:03.851417Z","iopub.status.idle":"2022-12-05T20:53:14.054257Z","shell.execute_reply.started":"2022-12-05T20:53:03.851378Z","shell.execute_reply":"2022-12-05T20:53:14.053214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# get the data ( super-slow if CPU )\nds_train = get_train_dataset()\n\nx_train = get_train_dataset_preview( ordered = True )   # x_train length is infinite\ny_train = next(iter(x_train.unbatch().map(lambda image, label: label).batch(NUM_TRAIN_IMG))).numpy()\nprint( 'train', len(y_train), y_train )\n# 12753 - array([ 93,   8,  86, ..., 102,  70,  45], dtype=int32)  # train shows up in random order\n# 104 different classes, so on average ~122 of each class \n\nds_valid = get_valid_dataset()  # ds_valid length is infinite\ny_valid = next(iter(ds_valid.unbatch().map(lambda image, label: label).batch(NUM_VALID_IMG))).numpy()\nprint( 'valid', len(y_valid), y_valid )\n# 3712 - array([13, 88, 53, ...,  8, 12, 67], dtype=int32) # valid is consistent order\n\nds_test = get_test_dataset()\n#display( 'test', len( ds_test ) )\nprint( 'test', ds_test.cardinality().numpy() )\n\n# can decorate something with @tf.autograph.experimental.do_not_convert to silence the warning","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:53:14.055892Z","iopub.execute_input":"2022-12-05T20:53:14.0562Z","iopub.status.idle":"2022-12-05T20:54:40.903589Z","shell.execute_reply.started":"2022-12-05T20:53:14.056165Z","shell.execute_reply":"2022-12-05T20:54:40.902438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# can we iterate over each dataset (& do something? e.g. pick colors, frequency analysis, etc.)","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:54:40.90526Z","iopub.execute_input":"2022-12-05T20:54:40.90557Z","iopub.status.idle":"2022-12-05T20:54:40.910454Z","shell.execute_reply.started":"2022-12-05T20:54:40.905534Z","shell.execute_reply":"2022-12-05T20:54:40.909359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"def batch_to_numpy( data ):\n    '''Converts batch of data to numpy '''\n    image , label = data\n    #print( 'batch_to_numpy() label', label, label.numpy() )\n    image = image.numpy()\n    label = label.numpy()\n    if label.dtype == object:\n        label = [None for _ in enumerate(label)]\n    return image , label","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:54:40.911876Z","iopub.execute_input":"2022-12-05T20:54:40.912236Z","iopub.status.idle":"2022-12-05T20:54:40.924181Z","shell.execute_reply.started":"2022-12-05T20:54:40.912185Z","shell.execute_reply":"2022-12-05T20:54:40.923119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/code/lovemm/petals-to-the-metal\n# what does it mean if title is blank\ndef title_from_label_and_target( label, correct_label ):\n    if correct_label is None:\n        return CLASSES[label], True\n    correct = (label == correct_label)\n    return \"{} [{}{}{}]\".format(CLASSES[label], 'OK' if correct else 'NO', u\"\\u2192\" if not correct else '',\n                                CLASSES[correct_label] if not correct else ''), correct\n\ndef display_batch_images( databatch, predictions = None ):\n    '''Plots Some Train , Test , Validation data'''\n    # load data as numpy \n    img, labels = batch_to_numpy( databatch )\n    if labels is None:\n        labels = [None for _ in enumerate(images)]\n    \n    rows = int( math.sqrt( len( img ) ) )\n    cols = len(img)//rows\n    # size and spacing\n    FIGSIZE = 16.0\n    SPACING = 0.11\n    subplot = ( rows, cols, 1 )\n    if rows < cols:\n        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n    else:\n        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n        \n    for i , (image, label) in enumerate(zip(img[:rows*cols],labels[:rows*cols])):\n        #title = '** DEFAULT **'\n        title = '?? None' if label is None else f'L:{label} - {CLASSES[label]}'\n        #print( label, CLASSES[label], title )\n        correct = True\n        if predictions is not None:\n            title, correct = title_from_label_and_target( predictions[i], label )\n        plt.subplot(rows,cols,i+1)\n        plt.axis('off')\n        plt.imshow(image)\n        if title:  # was if label but label could be 0\n            plt.title( title, fontsize = 9, color = 'black' if correct else 'red'  )  # CLASSES[label]\n        plt.tight_layout()\n        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n    plt.show()\n        ","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:54:40.925726Z","iopub.execute_input":"2022-12-05T20:54:40.926096Z","iopub.status.idle":"2022-12-05T20:54:40.94007Z","shell.execute_reply.started":"2022-12-05T20:54:40.926046Z","shell.execute_reply":"2022-12-05T20:54:40.939177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import scipy.cluster\nimport sklearn.cluster\nimport numpy\nfrom PIL import Image\n\ndef dominant_colors( image, cluster_count = 10, verbose = 0 ):  # PIL image input\n    # this counts the most common colors but we want the most-important colors\n\n    #image = image.resize((150, 150))      # optional, to reduce time\n    ar = numpy.asarray(image)  # convert input to a 1D array\n    shape = ar.shape\n    ar = ar.reshape( numpy.product( shape[:2] ), shape[2] ).astype(float)\n    print( shape, ar.shape )  # might convert from (192,192,3) to (36864,3 ) e.g. from 2D to 1D\n\n    kmeans = sklearn.cluster.MiniBatchKMeans(\n        n_clusters = cluster_count,\n        init = \"k-means++\",\n        max_iter = 20,\n        random_state = 1000,\n        verbose = verbose\n    ).fit(ar)\n    codes = kmeans.cluster_centers_\n    print( 'codes', codes )\n    \n    myBirch = sklearn.cluster.Birch(\n        n_clusters = cluster_count\n    ).fit( ar )\n    codes2 = myBirch.subcluster_centers_\n    print( 'codes2', codes2 )\n\n    vecs, _dist = scipy.cluster.vq.vq( ar, codes )         # assign codes\n    counts, _bins = numpy.histogram( vecs, len(codes) )    # count occurrences\n    print( 'counts, _bins', counts, _bins )\n\n    colors = []\n    for index in numpy.argsort( counts )[::-1]:\n        #colors.append( tuple( [ int(code) for code in codes[index]]))\n        colors.append( tuple( [ int( 255 * code ) for code in codes[index]]))\n    return colors                    # returns colors in order of dominance","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:54:40.941376Z","iopub.execute_input":"2022-12-05T20:54:40.941713Z","iopub.status.idle":"2022-12-05T20:54:41.052264Z","shell.execute_reply.started":"2022-12-05T20:54:40.941669Z","shell.execute_reply":"2022-12-05T20:54:41.051214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# analyze a single image\n# probably should convert to HSV space first & then perhaps to 1+ dimension ( H+ )\n# also need to recognize that colors hues are cyclic number\nimage, label = batch_to_numpy( next( iter( ds_train.unbatch().batch( 1 ) ) ) )\nprint( label )\nplt.imshow( image[0] ) # Invalid shape (1, 192, 192, 3) for image data\nplt.show()\nmydominant_colors = dominant_colors( image[0], 7 )\nprint( mydominant_colors )\n\npalette = np.array(mydominant_colors)[np.newaxis, :, :]\n\nplt.imshow(palette)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:54:41.055064Z","iopub.execute_input":"2022-12-05T20:54:41.055346Z","iopub.status.idle":"2022-12-05T20:54:53.458379Z","shell.execute_reply.started":"2022-12-05T20:54:41.055317Z","shell.execute_reply":"2022-12-05T20:54:53.457247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_batch_images( next( iter( ds_train.unbatch().batch( 6 * 6 ) ) ) )\n# training data is 'augmented'","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:54:53.460071Z","iopub.execute_input":"2022-12-05T20:54:53.461107Z","iopub.status.idle":"2022-12-05T20:55:06.252198Z","shell.execute_reply.started":"2022-12-05T20:54:53.461031Z","shell.execute_reply":"2022-12-05T20:55:06.251017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_batch_images( next( iter( ds_valid.unbatch().batch( 6 * 6 ) ) ) )\n# validation data is not augmented","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:55:06.253831Z","iopub.execute_input":"2022-12-05T20:55:06.254259Z","iopub.status.idle":"2022-12-05T20:55:12.742483Z","shell.execute_reply.started":"2022-12-05T20:55:06.254204Z","shell.execute_reply":"2022-12-05T20:55:12.741058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_batch_images( next( iter( ds_test.unbatch().batch( 6 * 6 ) ) ) )","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:55:12.744538Z","iopub.execute_input":"2022-12-05T20:55:12.744974Z","iopub.status.idle":"2022-12-05T20:55:21.632615Z","shell.execute_reply.started":"2022-12-05T20:55:12.744939Z","shell.execute_reply":"2022-12-05T20:55:21.62844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\n# turn this horizontonally & show both ( as percentages )\n\ntrain_agg = np.asarray([[label, (y_train == index).sum()] for index, label in enumerate(CLASSES)])\nvalid_agg = np.asarray([[label, (y_valid == index).sum()] for index, label in enumerate(CLASSES)])\n#print( train_agg )  # seems to be array of pairs, label, count ( as a string )\n#print( valid_agg )\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(24, 64))\n\ncount_vals = [ eval(i) for i in train_agg[...,1] ]\nlabel_vals = train_agg[...,0]\nax1 = sns.barplot(x=count_vals, y= label_vals , order=CLASSES, ax=ax1)\nax1.set_title('Train', fontsize=20)\nax1.tick_params( labelsize=10 )\n\ncount_vals = [ eval(i) for i in valid_agg[...,1] ]\nlabel_vals = valid_agg[...,0]\nax2 = sns.barplot(x=count_vals, y= label_vals, order=CLASSES, ax=ax2)\nax2.set_title('Validation', fontsize=20)\nax2.tick_params( labelsize=10 )\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:55:21.634483Z","iopub.execute_input":"2022-12-05T20:55:21.634865Z","iopub.status.idle":"2022-12-05T20:55:26.49348Z","shell.execute_reply.started":"2022-12-05T20:55:21.634826Z","shell.execute_reply":"2022-12-05T20:55:26.492231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ! Data is multi-class and unbalanced ( imbalance ) ( if train_data is augmented with tf-flower-photo-tfrec, then train distribution <> valid distribution )\n* Two common techniques\n * Class Weights - classifier gives greater weight to labels in minority classes\n * Oversampling ( of minority classes )","metadata":{}},{"cell_type":"code","source":"%%time\n\n# from sklearn.utils import class_weight\n# my_class_weights = class_weight.compute_class_weight(\n#     'balanced'\n#     , np.unique(y_train)\n#     , y_train\n# )\n# my_class_weights = dict(zip(np.unique(y_train), class_weight.compute_class_weight('balanced', np.unique(y_train), y_train))) \n\nweight_per_class = True\n\nif weight_per_class:\n    from collections import Counter\n    import gc\n\n    gc.enable()\n\n    def get_training_dataset_raw():\n        dataset = load_dataset(TRAINING_IMAGES, is_labeled = True, inorder = False)\n        return dataset\n\n    raw_training_dataset = get_training_dataset_raw()\n\n    label_counter = Counter()\n    for images, labels in raw_training_dataset:\n        label_counter.update([labels.numpy()])\n\n    class_count = len( label_counter )\n    del raw_training_dataset\n    print( 'label_counter', label_counter )  # seems to be id, count\n\n    TARGET_NUM_PER_CLASS = 122 #??\n\n    def get_weight_for_class(class_id):\n        counting = label_counter[class_id]\n        weight = TARGET_NUM_PER_CLASS / counting\n        return weight\n\n    weight_per_class = {class_id: get_weight_for_class(class_id) for class_id in range( class_count )} # dictionary of id, float\n    print( weight_per_class)","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:55:26.494808Z","iopub.execute_input":"2022-12-05T20:55:26.495526Z","iopub.status.idle":"2022-12-05T20:56:42.906499Z","shell.execute_reply.started":"2022-12-05T20:55:26.495486Z","shell.execute_reply":"2022-12-05T20:56:42.905423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_class_weight( labels_dict, mu = 0.15 ):\n    total = np.sum( list( labels_dict.values() ) )\n    keys = labels_dict.keys()\n    \n    class_weight = dict()\n    \n    for key in keys:\n        score = math.log( mu * total / float( labels_dict[ key ] ) )\n        class_weight[key] = score if score > 1.0 else 1.0  # trims bottom ( most-common ) to 1.0\n    \n    return class_weight\n\nclass_count = len( label_counter )\nlabels_dict = {class_id: label_counter[ class_id ]  for class_id in range( class_count )}\nprint( labels_dict )\nclass_weight_dict = create_class_weight( labels_dict )\nprint( class_weight_dict )","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:56:42.907879Z","iopub.execute_input":"2022-12-05T20:56:42.908189Z","iopub.status.idle":"2022-12-05T20:56:42.917693Z","shell.execute_reply.started":"2022-12-05T20:56:42.908153Z","shell.execute_reply":"2022-12-05T20:56:42.916648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nfrom matplotlib import cm\n\nif weight_per_class:\n    df_class = pd.DataFrame.from_dict(weight_per_class, orient='index', columns=['class_weight'])\n    display( df_class )\n    plt.figure(figsize=(35, 6))\n\n    #barplot color based on value\n    bplot = sns.barplot(x=df_class.index, y='class_weight', data=df_class, palette= cm.Blues(df_class['class_weight']*0.15));\n    for p in bplot.patches:\n        bplot.annotate(format(p.get_height(), '.1f'), \n                       (p.get_x() + p.get_width() / 2., p.get_height()), \n                       ha = 'center', va = 'center', \n                       xytext = (0, 9), \n                       textcoords = 'offset points')\n    plt.xlabel(\"Class\", size=14)\n    plt.ylabel(\"Class weight (inverse of %)\", size=14)","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:56:42.91903Z","iopub.execute_input":"2022-12-05T20:56:42.919376Z","iopub.status.idle":"2022-12-05T20:56:45.236135Z","shell.execute_reply.started":"2022-12-05T20:56:42.919345Z","shell.execute_reply":"2022-12-05T20:56:45.235119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if weight_per_class:\n    df_class = pd.DataFrame.from_dict(class_weight_dict, orient='index', columns=['class_weight'])\n    #display( df_class )\n    plt.figure(figsize=(35, 6))\n\n    #barplot color based on value\n    bplot = sns.barplot(x=df_class.index, y='class_weight', data=df_class, palette= cm.Blues(df_class['class_weight']*0.15));\n    for p in bplot.patches:\n        bplot.annotate(format(p.get_height(), '.1f'), \n                       (p.get_x() + p.get_width() / 2., p.get_height()), \n                       ha = 'center', va = 'center', \n                       xytext = (0, 9), \n                       textcoords = 'offset points')\n    plt.xlabel(\"Class\", size=14)\n    plt.ylabel(\"Class weight (inverse of %)\", size=14)","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:56:45.237555Z","iopub.execute_input":"2022-12-05T20:56:45.237821Z","iopub.status.idle":"2022-12-05T20:56:47.4978Z","shell.execute_reply.started":"2022-12-05T20:56:45.237781Z","shell.execute_reply":"2022-12-05T20:56:47.496879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling & tuning ","metadata":{"execution":{"iopub.status.busy":"2022-11-10T07:22:01.114433Z","iopub.execute_input":"2022-11-10T07:22:01.114716Z","iopub.status.idle":"2022-11-10T07:22:02.85957Z","shell.execute_reply.started":"2022-11-10T07:22:01.114686Z","shell.execute_reply":"2022-11-10T07:22:02.858468Z"}}},{"cell_type":"markdown","source":"# Generate Embeddings ( from a pre-trained model )\n * Notebook: Imagenet embeddings+RAPIDS SVR+Finetuned models\n   * https://www.kaggle.com/code/titericz/imagenet-embeddings-rapids-svr-finetuned-models","metadata":{}},{"cell_type":"code","source":"DO_EMBEDDINGS1 = False # this is a language-model embeddings pattern\nif ( DO_EMBEDDINGS1 ) :\n    from transformers import AutoModel, AutoTokenizer\n    import torch\n    import torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:56:47.504604Z","iopub.execute_input":"2022-12-05T20:56:47.505368Z","iopub.status.idle":"2022-12-05T20:56:47.510794Z","shell.execute_reply.started":"2022-12-05T20:56:47.505332Z","shell.execute_reply":"2022-12-05T20:56:47.51001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if ( DO_EMBEDDINGS1 ) :\n    def mean_pooling( model_output, attention_mask ):\n        # last_hidden_state (torch.FloatTensor of shape (batch_size, sequence_length, hidden_size)) — Sequence of hidden-states at the output of the last layer of the model.\n        token_embeddings = model_output.last_hidden_state.detach().cpu()\n\n        input_mask_expanded = (\n            attention_mask.unsqueeze(-1).expand( token_embeddings.size() ).float()\n        )\n\n        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp( input_mask_expanded.sum(1), min=1e-9 )\n\n    class EmbedDataset(torch.utils.data.Dataset):\n        def __init__(self,df):\n            self.df = df.reset_index(drop=True)\n\n        def __len__(self):\n            return len(self.df)\n\n        def __getitem__(self,idx):\n            text = self.df.loc[idx,\"full_text\"]\n            tokens = tokenizer(\n                    text,\n                    None,\n                    add_special_tokens=True,\n                    padding='max_length',\n                    truncation = True,\n                    max_length = MAX_LEN\n                , return_tensors = \"pt\")\n            tokens = {k:v.squeeze(0) for k,v in tokens.items()}\n            return tokens\n\n    BATCH_SIZE = 4  # why this size?  This is the number of samples processed before the model is updated\n# ds_tr = EmbedDataset(dftr)\n# embed_dataloader_tr = torch.utils.data.DataLoader(ds_tr,\\\n#                         batch_size=BATCH_SIZE,\\\n#                         shuffle=False)\n# ds_te = EmbedDataset(dfte)\n# embed_dataloader_te = torch.utils.data.DataLoader(ds_te,\\\n#                         batch_size=BATCH_SIZE,\\\n#                         shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:56:47.512192Z","iopub.execute_input":"2022-12-05T20:56:47.512502Z","iopub.status.idle":"2022-12-05T20:56:47.524909Z","shell.execute_reply.started":"2022-12-05T20:56:47.512461Z","shell.execute_reply":"2022-12-05T20:56:47.523792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if ( DO_EMBEDDINGS1 ) :\n    tokenizer = None\n    MAX_LEN = 640\n\n    # we might instead want to get not the output of last layer but some other\n    # https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n    def get_embeddings( MODEL_NM = '', MAX = 640, BATCH_SIZE = 4, verbose = True ):\n        global tokenizer, MAX_LEN\n        DEVICE = \"cuda\"\n        model = AutoModel.from_pretrained( MODEL_NM ) # output_hidden_states = True\n        #model = AutoModel.from_pretrained( MODEL_NM, output_hidden_states = True ) \n        tokenizer = AutoTokenizer.from_pretrained( MODEL_NM )\n        MAX_LEN = MAX\n\n        model = model.to( DEVICE )\n        model.eval() # put model in 'evaluation mode' ( feed-forward operation ) (? as compared with training/tuning mode )\n\n        all_train_text_feats = []\n        for batch in tqdm( embed_dataloader_tr, total = len( embed_dataloader_tr ) ):\n            # convert to tensors\n            input_ids = batch[\"input_ids\"].to(DEVICE)\n            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n            with torch.no_grad():\n                model_output = model( input_ids = input_ids, attention_mask = attention_mask )\n                #model_outputs = model( input_ids = input_ids, attention_mask = attention_mask )\n                # hidden_states = model_outputs[2]\n                # token_embeddings = torch.stack( hidden_states, dim = 0 ) # Concatenate the tensors for all layers. Use 'stack' to add new dimension\n\n            sentence_embeddings = mean_pooling( model_output, attention_mask.detach().cpu() )\n            # Normalize the embeddings (? why)\n            sentence_embeddings = F.normalize( sentence_embeddings, p=2.0, dim=1 ) # perform Lp normalization of input over specified dimension\n            sentence_embeddings = sentence_embeddings.squeeze(0).detach().cpu().numpy()\n            all_train_text_feats.extend( sentence_embeddings )\n        all_train_text_feats = np.array( all_train_text_feats )\n        if verbose:\n            print('Train embeddings shape',all_train_text_feats.shape)\n\n        te_text_feats = []\n        for batch in tqdm( embed_dataloader_te, total = len( embed_dataloader_te ) ):\n            input_ids = batch[\"input_ids\"].to(DEVICE)\n            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n            with torch.no_grad():\n                model_output = model( input_ids = input_ids, attention_mask = attention_mask )\n            sentence_embeddings = mean_pooling( model_output, attention_mask.detach().cpu())\n            # Normalize the embeddings\n            sentence_embeddings = F.normalize( sentence_embeddings, p=2.0, dim=1 )\n            sentence_embeddings = sentence_embeddings.squeeze(0).detach().cpu().numpy()\n            te_text_feats.extend( sentence_embeddings )\n        te_text_feats = np.array( te_text_feats )\n        if verbose:\n            print('Test embeddings shape',te_text_feats.shape)\n\n        return all_train_text_feats, te_text_feats","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:56:47.526623Z","iopub.execute_input":"2022-12-05T20:56:47.526928Z","iopub.status.idle":"2022-12-05T20:56:47.542787Z","shell.execute_reply.started":"2022-12-05T20:56:47.526887Z","shell.execute_reply":"2022-12-05T20:56:47.541702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"USE_timm = False\nif ( USE_timm ) :\n    !pip install timm  # https://timm.fast.ai/","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:56:47.544304Z","iopub.execute_input":"2022-12-05T20:56:47.544594Z","iopub.status.idle":"2022-12-05T20:56:47.558386Z","shell.execute_reply.started":"2022-12-05T20:56:47.544561Z","shell.execute_reply":"2022-12-05T20:56:47.557426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use Multi-Model via timm","metadata":{}},{"cell_type":"code","source":"if ( USE_timm ) :\n    import timm\n    avail_pretrained_models = timm.list_models( pretrained = True )\n# print( len(avail_pretrained_models), avail_pretrained_models )  # 770 models","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:56:47.559761Z","iopub.execute_input":"2022-12-05T20:56:47.560028Z","iopub.status.idle":"2022-12-05T20:56:47.570338Z","shell.execute_reply.started":"2022-12-05T20:56:47.559992Z","shell.execute_reply":"2022-12-05T20:56:47.569112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if ( USE_timm ) :\n    # select some subset of pretrained models\n    names = [\n        'deit_base_distilled_patch16_384',\n        #'fbnetc_100',\n        #'ig_resnext101_32x8d',\n        'ig_resnext101_32x48d',\n        'repvgg_b0',\n        'resnetv2_152x4_bitm',\n        #'rexnet_200',\n        #'resnest269e',\n        'swsl_resnext101_32x8d',\n        #'tf_efficientnet_b6_ns',\n        #'tf_efficientnet_b7_ns',\n        #'tf_efficientnet_b8_ap',\n        'tf_efficientnet_l2_ns_475',\n        'vit_base_patch16_384',\n        #'vit_large_patch16_384',\n        'vit_large_r50_s32_384',\n    ]\n\n    names_hflip_crop = [\n        'tf_efficientnet_l2_ns_hflip_384',\n        'deit_base_distilled_patch16_384_hflip_384',\n        'ig_resnext101_32x48d_hflip_384',\n        'tf_efficientnet_l2_ns_512',\n    ]\n\n    names_orig = [\n        'ig_resnext101_32x48d',\n        'vit_large_r50_s32_384',\n        'clip_RN50x4',\n        'clip_ViT-B-16',\n        'clip_RN50x16',\n        'clip_ViT-B-32',\n    ]","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:56:47.57176Z","iopub.execute_input":"2022-12-05T20:56:47.572379Z","iopub.status.idle":"2022-12-05T20:56:47.582549Z","shell.execute_reply.started":"2022-12-05T20:56:47.572334Z","shell.execute_reply":"2022-12-05T20:56:47.581515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from glob import glob\n\n# modelpath = { m.split('/')[-1].split('.')[0] :m for m in glob('../input/pytorch-pretrained-0/*.pt')+glob('../input/pytorch-pretrained-1/*.pt')+glob('../input/pytorch-pretrained-2/*.pt')+glob('../input/pytorch-pretrained-3/*.pt')}\n# modelpath  # empty since they are not in local 'input' drive","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:56:47.584278Z","iopub.execute_input":"2022-12-05T20:56:47.584776Z","iopub.status.idle":"2022-12-05T20:56:47.59735Z","shell.execute_reply.started":"2022-12-05T20:56:47.584743Z","shell.execute_reply":"2022-12-05T20:56:47.596404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:56:47.598879Z","iopub.execute_input":"2022-12-05T20:56:47.599158Z","iopub.status.idle":"2022-12-05T20:56:48.670184Z","shell.execute_reply.started":"2022-12-05T20:56:47.599125Z","shell.execute_reply":"2022-12-05T20:56:48.669019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if ( USE_timm ) :\n    # create an individual model\n    myModel = timm.create_model( 'resnet34', pretrained = True ) # num_classes = 104 + 1\n    x     = torch.randn(1, 3, 224, 224)\n    myModel(x).shape  # output is routinely 1000","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:56:48.67174Z","iopub.execute_input":"2022-12-05T20:56:48.672841Z","iopub.status.idle":"2022-12-05T20:56:48.678561Z","shell.execute_reply.started":"2022-12-05T20:56:48.672787Z","shell.execute_reply":"2022-12-05T20:56:48.677472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"USE_fastai = False\nif ( USE_fastai ) :\n    from fastai.vision.all import *  # for get_image_files()\n\n    #path = untar_data(URLs.PETS)/'images'\n    # bears = DataBlock(\n    #     blocks=(ImageBlock, CategoryBlock), \n    #     get_items=get_image_files, \n    #     splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    #     get_y=parent_label,\n    #     item_tfms=Resize(128))\n\n    # TRAINING_IMAGES = tf.io.gfile.glob( f'{GCS_PATH}/train/*.tfrec' ) # glob() returns a list of files that match the given patterns\n    # VALID_IMAGES = tf.io.gfile.glob( f'{GCS_PATH}/val/*.tfrec' )\n    # TEST_IMAGES = tf.io.gfile.glob( f'{GCS_PATH}/test/*.tfrec' )\n\n    #result = get_image_files( GCS_PATH, recurse=True, folders=None ) \n    #result = get_files( f'{GCS_PATH}/train/', extensions = '.tfrec', recurse=True, folders=None ) \n    #print( result ) # []\n    result = tf.io.gfile.glob( f'{GCS_PATH}/train/*.tfrec' )\n    print( result )\n\n    dls = ImageDataLoaders.from_name_func(\n        GCS_PATH\n        , get_image_files( GCS_PATH )  # def get_image_files(path, recurse=True, folders=None):\n        #, tf.io.gfile.glob( f'{GCS_PATH}/train/*.tfrec' )\n        , valid_pct=0.2\n        , label_func=lambda x: x[0].isupper()\n        , item_tfms = Resize(224)\n    )\n\n    # if a string is passed into the model argument, it will now use timm (if it is installed)\n    learn = vision_learner(dls, 'vit_tiny_patch16_224', metrics=error_rate)\n\n    learn.fine_tune(1)","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:56:48.680106Z","iopub.execute_input":"2022-12-05T20:56:48.680544Z","iopub.status.idle":"2022-12-05T20:56:48.691422Z","shell.execute_reply.started":"2022-12-05T20:56:48.680502Z","shell.execute_reply":"2022-12-05T20:56:48.690419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q efficientnet","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:56:48.692535Z","iopub.execute_input":"2022-12-05T20:56:48.692959Z","iopub.status.idle":"2022-12-05T20:56:58.65168Z","shell.execute_reply.started":"2022-12-05T20:56:48.692927Z","shell.execute_reply":"2022-12-05T20:56:58.650305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define custom loss functions ( used at model-compile-time )","metadata":{}},{"cell_type":"code","source":"# define a custom loss function that is similar to f1 ( but f1 is not differentiable )\n# since the model-submission is being evaluated on f1, best if we use it directly ( instead of sparse_categorical_accuracy )\n#https://www.kaggle.com/code/rejpalcz/best-loss-function-for-f1-score-metric/notebook\n    \ndef f1( y_true, y_pred ):\n    y_pred = K.round(y_pred)\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    f1 = 2*p*r / (p+r+K.epsilon())\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)\n\ndef f1_loss( y_true, y_pred ):  # this seems to be just 1 - f1 ( so it is 'loss' and should be minimized )\n    \n#     tp = K.sum( K.cast( y_true * y_pred, 'float'), axis=0)\n#     tn = K.sum( K.cast( ( 1 - y_true ) * ( 1 - y_pred ), 'float'), axis=0)\n#     fp = K.sum( K.cast( ( 1 - y_true ) * y_pred, 'float'), axis=0)\n#     fn = K.sum( K.cast( y_true * ( 1 - y_pred ), 'float'), axis=0)\n\n    y_true_float = K.cast( y_true, 'float' )\n    y_pred_float = K.cast( y_pred, 'float' )\n    tp = K.sum( y_true_float * y_pred_float, axis=0)\n    tn = K.sum( ( 1 - y_true_float ) * ( 1 - y_pred_float ), axis=0)\n    fp = K.sum( ( 1 - y_true_float ) * y_pred_float, axis=0)\n    fn = K.sum( y_true_float * ( 1 - y_pred_float ), axis=0)\n\n    p = tp / ( tp + fp + K.epsilon() )\n    r = tp / ( tp + fn + K.epsilon() )\n\n    f1 = 2 * p * r / ( p + r + K.epsilon() )\n    f1 = tf.where( tf.math.is_nan( f1 ), tf.zeros_like( f1 ), f1 )\n    return 1 - K.mean( f1 )","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:56:58.654257Z","iopub.execute_input":"2022-12-05T20:56:58.654683Z","iopub.status.idle":"2022-12-05T20:56:58.669482Z","shell.execute_reply.started":"2022-12-05T20:56:58.654629Z","shell.execute_reply":"2022-12-05T20:56:58.668237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Define F1 measures: F1 = 2 * (precision * recall) / (precision + recall)\n# https://neptune.ai/blog/implementing-the-macro-f1-score-in-keras\n# this perhaps dosen't 'average' / normalize the f1 score\n\ndef custom_f1( y_true, y_pred ):\n    def recall_m( y_true, y_pred ):\n        TP = K.sum( K.round( K.clip( y_true * y_pred, 0, 1)))\n        Positives = K.sum( K.round( K.clip( y_true, 0, 1))) # real positives\n\n        recall = TP / ( Positives + K.epsilon())\n        return recall\n\n\n    def precision_m( y_true, y_pred ):\n        TP = K.sum( K.round( K.clip( y_true * y_pred, 0, 1)))\n        Pred_Positives = K.sum( K.round( K.clip( y_pred, 0, 1)))\n\n        precision = TP / ( Pred_Positives + K.epsilon())  # ?precision could be greater than 1\n        return precision\n\n    precision, recall = precision_m( y_true, y_pred ), recall_m( y_true, y_pred )\n\n    return 2 * ( ( precision * recall ) / ( precision + recall + K.epsilon() ) )\n\n# can we test custom_f1() here?\n# print( custom_f1( [ 0 ], [ 0 ] ) )\n# print( custom_f1( [ 1 ], [ 1 ] ) )\n# print( custom_f1( [ 0 ], [ 1 ] ) )\n# print( custom_f1( [ 1 ], [ 0 ] ) )\n# print( custom_f1( [ 1, 0 ], [ 1, 1 ] ) )","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:56:58.67177Z","iopub.execute_input":"2022-12-05T20:56:58.67221Z","iopub.status.idle":"2022-12-05T20:56:58.686113Z","shell.execute_reply.started":"2022-12-05T20:56:58.672128Z","shell.execute_reply":"2022-12-05T20:56:58.685195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = 1\nb = 0\n# precision = TP / PP\n# recall = TP / all_positives\n# f1 = 2 * P * R / ( P + R )\nprint( f1_loss( torch.FloatTensor( [ a, a ] ), torch.FloatTensor( [ b, b ] ) ) ) # unsupported operand type(s) for -: 'int' and 'list'\nprint( f1( torch.FloatTensor( [ a, a ] ), torch.FloatTensor( [ b, b ] ) ) ) # unsupported operand type(s) for -: 'int' and 'list'\nprint( custom_f1( torch.FloatTensor( [ a, a ] ), torch.FloatTensor( [ b, b ] ) ) ) # unsupported operand type(s) for -: 'int' and 'list'\n","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:56:58.687458Z","iopub.execute_input":"2022-12-05T20:56:58.687756Z","iopub.status.idle":"2022-12-05T20:56:58.772355Z","shell.execute_reply.started":"2022-12-05T20:56:58.687723Z","shell.execute_reply":"2022-12-05T20:56:58.771225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# could define custom Callback here, then add it to callbacks in .fit()\n# https://www.tensorflow.org/guide/keras/custom_callback\n# would like to: perhaps stop if val_loss too bouncy\n#  also predict (using curve-fit) threshold asymptope \n# perhaps predict_proba of unknown-test (after some level) & then use some trend or median\n#https://neptune.ai/blog/implementing-the-macro-f1-score-in-keras\n    \nclass myCallbackClass( tf.keras.callbacks.Callback ):\n    \n    def __init__(self, patience=0):\n        #super(EarlyStoppingAtMinLoss, self).__init__()  # call some parent class\n        self.patience = patience  # store the initialization value\n        # best_weights to store the weights at which the minimum loss occurs.\n        self.best_weights = None  # initialize \n        \n    #def on_train_begin(self, logs=None):\n    def on_train_begin(self, logs={}):\n        self._data = []\n        self.val_f1s = []\n        self.val_recalls = []\n        self.val_precisions = []\n        keys = list(logs.keys())\n        print(\"Starting training; got log keys: {}\".format(keys))\n\n    def on_train_end(self, logs=None):\n        keys = list(logs.keys())\n        print(\"Stop training; got log keys: {}\".format(keys))\n\n    def on_epoch_begin(self, epoch, logs=None):\n        keys = list(logs.keys())\n        print(\"Start epoch {} of training; got log keys: {}\".format(epoch, keys))\n\n    #def on_epoch_end(self, epoch, logs=None):\n    def on_epoch_end(self, batch, logs={}):\n        keys = list(logs.keys())\n        print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n        X_val, y_val = self.validation_data[0], self.validation_data[1]\n        y_predict = np.asarray( self.model.predict(X_val))\n\n        y_val = np.argmax(y_val, axis=1)\n        y_predict = np.argmax(y_predict, axis=1)\n\n        print(f' — val_f1: {val_f1} — val_precision: {val_precision}, — val_recall: {val_recall}')\n\n        self._data.append({\n            'val_rocauc': roc_auc_score(y_val, y_predict),\n        })\n        return\n    \n    def on_test_begin(self, logs=None):\n        keys = list(logs.keys())\n        print(\"Start testing; got log keys: {}\".format(keys))\n\n    def on_test_end(self, logs=None):\n        keys = list(logs.keys())\n        print(\"Stop testing; got log keys: {}\".format(keys))\n\n    def on_predict_begin(self, logs=None):\n        keys = list(logs.keys())\n        print(\"Start predicting; got log keys: {}\".format(keys))\n\n    def on_predict_end(self, logs=None):\n        keys = list(logs.keys())\n        print(\"Stop predicting; got log keys: {}\".format(keys))\n\n    def on_train_batch_begin(self, batch, logs=None):\n        keys = list(logs.keys())\n        print(\"...Training: start of batch {}; got log keys: {}\".format(batch, keys))\n\n    def on_train_batch_end(self, batch, logs=None):\n        keys = list(logs.keys())\n        print(\"...Training: end of batch {}; got log keys: {}\".format(batch, keys))\n        print(\n            \"Up to batch {}, the average loss is {:7.2f}.\".format(batch, logs[\"loss\"])\n        )\n\n    def on_test_batch_begin(self, batch, logs=None):\n        keys = list(logs.keys())\n        print(\"...Evaluating: start of batch {}; got log keys: {}\".format(batch, keys))\n\n    def on_test_batch_end(self, batch, logs=None):\n        keys = list(logs.keys())\n        print(\"...Evaluating: end of batch {}; got log keys: {}\".format(batch, keys))\n\n    def on_predict_batch_begin(self, batch, logs=None):\n        keys = list(logs.keys())\n        print(\"...Predicting: start of batch {}; got log keys: {}\".format(batch, keys))\n\n    def on_predict_batch_end(self, batch, logs=None):\n        keys = list(logs.keys())\n        print(\"...Predicting: end of batch {}; got log keys: {}\".format(batch, keys))    \n\n    def get_data(self):\n        return self._data\n    \nmyCallback = myCallbackClass()","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:56:58.773808Z","iopub.execute_input":"2022-12-05T20:56:58.774171Z","iopub.status.idle":"2022-12-05T20:56:58.797745Z","shell.execute_reply.started":"2022-12-05T20:56:58.774127Z","shell.execute_reply":"2022-12-05T20:56:58.796581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Defining the Callback Metrics Object to track in Neptune\nclass myCallbackClass2( tf.keras.callbacks.Callback ):\n    def __init__(self, validation, current_fold):\n        #super(NeptuneMetrics, self).__init__()\n        #self.exp = neptune_experiment\n        self.validation = validation  # could be dataset not X_train, y_train\n        self.curFold = current_fold\n\n    def on_train_begin(self, logs={}):\n        self.val_f1s = []\n        self.val_recalls = []\n        self.val_precisions = []\n        self.val_accuracy = []\n\n    def on_epoch_end(self, epoch, logs={}):\n        \n        # something like:\n        y_valid = next(iter(self.validation.unbatch().map(lambda image, label: label).batch(NUM_VALID_IMG))).numpy()\n        x_valid = self.validation.map( lambda image, label: image )\n        #display( 'on_epoch_end() valid: ', len( y_valid ), len( x_valid ) )\n        #print( 'on_epoch_end() valid: ', len( y_valid ) )\n        valid_predict = model.predict( x_valid )\n        valid_preds = np.argmax( valid_predict, axis = -1 )\n\n        decimal_places = 4\n        average_method = 'macro'\n        val_f1 = round( sklearn.metrics.f1_score( y_true = y_valid, y_pred = valid_preds, average = average_method ), decimal_places)\n        val_recall = round( sklearn.metrics.recall_score( y_true = y_valid, y_pred = valid_preds, average = average_method ), decimal_places)\n        val_precision = round( sklearn.metrics.precision_score( y_true = y_valid, y_pred = valid_preds, average = average_method ), decimal_places)\n        val_accuracy = round( sklearn.metrics.accuracy_score( y_true = y_valid, y_pred = valid_preds ), decimal_places)\n        # val_SparseCategoricalCrossentropy = This is a keras .fit metric, not a function to calculate\n\n        self.val_f1s.append(val_f1)\n        self.val_recalls.append(val_recall)\n        self.val_precisions.append(val_precision)\n        self.val_accuracy.append( val_accuracy )\n\n        print(f' validation: count: {len(y_valid)} — val_f1: {val_f1} — val_precision: {val_precision}, — val_recall: {val_recall}')\n\n        ### Send the performance metrics to Neptune for tracking (new version) ###\n        #self.exp['Epoch End Loss'].log(logs['loss'])\n        #self.exp['Epoch End F1-score'].log(val_f1)\n        #self.exp['Epoch End Precision'].log(val_precision)\n        #self.exp['Epoch End Recall'].log(val_recall)\n\n#         if self.curFold == 4:\n#             ### Log Epoch End metrics values for each step in the last CV fold ###\n#             msg = f' End of epoch {epoch} val_f1: {val_f1} — val_precision: {val_precision}, — val_recall: {val_recall}'\n#             ### Neptune new version\n#             self.exp[f'Epoch End Metrics (each step) for fold {self.curFold}'] = msg","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:56:58.799438Z","iopub.execute_input":"2022-12-05T20:56:58.799821Z","iopub.status.idle":"2022-12-05T20:56:58.814384Z","shell.execute_reply.started":"2022-12-05T20:56:58.799786Z","shell.execute_reply":"2022-12-05T20:56:58.813491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Custom Learning Rate Schedulers ( used at .fit() run-time )","metadata":{}},{"cell_type":"code","source":"#import kernel_tensorflow_utils as ktu\n# from https://www.kaggle.com/code/chankhavu/kernel-tensorflow-utils\nfrom abc import ABC, abstractmethod\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n\nclass LRSchedulers:\n    \"\"\"\n    A collection of convenient learning rate schedulers. Wrapped under this one big\n    class just for convenience (since this is an utility code file and we want to import\n    only one file, we will use classes as namespaces).\n    \n    The classes in LRSchedulers namespace SHOULD inherit from LRVisualizer and hence should\n    contain a method to visualize the learning rate.\n    \"\"\"\n\n    class LRVisualizer(ABC):\n        \"\"\"The base class for learning rate visualization\"\"\"\n        \n        @abstractmethod\n        def _lr_by_batchnum(self, steps_per_epoch, epochs):\n            pass\n        \n        def visualize(self, steps_per_epoch, epochs):\n            learning_rates = self._lr_by_batchnum(steps_per_epoch, epochs)\n            plt.plot(np.arange(steps_per_epoch * epochs), learning_rates)\n            plt.xlabel('batch number')\n            plt.ylabel('learning rate')\n            \n    class FineTuningLR(LearningRateScheduler, LRVisualizer):\n        \"\"\"\n        Starts small (to not ruin delicate pre-trained weights), increases, then decays exponentially.\n\n        # Arguments:\n            lr_start: the initial learning rate\n            lr_max: the peak learning rate\n            lr_min: the lowest learning rate at the end\n            lr_rampup_epochs: number of epochs before peak\n            lr_sustain_epochs: number of epochs at the peak\n            lr_exp_decay: exponential lr decay parameter\n            verbose: int. 0: quiet, 1: update messages.\n        \"\"\"\n        def __init__(self,\n                     lr_start=0.00001,\n                     lr_max=0.00005,\n                     lr_min=0.00001,\n                     lr_rampup_epochs=5,\n                     lr_sustain_epochs=0,\n                     lr_exp_decay=.8,\n                     verbose=0):\n\n            self.lr_start = lr_start\n            self.lr_max = lr_max\n            self.lr_min = lr_min\n            self.lr_rampup_epochs = lr_rampup_epochs\n            self.lr_sustain_epochs = lr_sustain_epochs\n            self.lr_exp_decay = lr_exp_decay\n\n            super().__init__(self.schedule, verbose)\n\n        def schedule(self, epoch):\n            if epoch < self.lr_rampup_epochs:\n                lr = (self.lr_max - self.lr_start) / self.lr_rampup_epochs * epoch + self.lr_start\n            elif epoch < self.lr_rampup_epochs + self.lr_sustain_epochs:\n                lr = self.lr_max\n            else:\n                lr = (self.lr_max - self.lr_min) * self.lr_exp_decay ** \\\n                (epoch - self.lr_rampup_epochs - self.lr_sustain_epochs) + self.lr_min\n            return lr\n\n        def _lr_by_batchnum(self, steps_per_epoch, epochs):\n            ret = np.zeros(shape=(steps_per_epoch * epochs))\n            for epoch in range(epochs):\n                ret[epoch * steps_per_epoch:(epoch + 1) * steps_per_epoch] = self.schedule(epoch)\n            return ret","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:56:58.816055Z","iopub.execute_input":"2022-12-05T20:56:58.816456Z","iopub.status.idle":"2022-12-05T20:56:58.832908Z","shell.execute_reply.started":"2022-12-05T20:56:58.816413Z","shell.execute_reply":"2022-12-05T20:56:58.832151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adaptive LR\nLR_START = 0.00005\nLR_MAX = LR_START \nLR_MIN = 0.00001 \nLR_RAMPUP_EPOCHS = 0 \nLR_SUSTAIN_EPOCHS = 5 \nLR_EXP_DECAY = 0.85\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:  \n        lr = LR_START + (epoch * (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS)   \n    elif epoch < (LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS):  \n        lr = LR_MAX\n    else:    \n        lr = LR_MIN + (LR_MAX - LR_MIN) * LR_EXP_DECAY ** (epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS)\n\n    return lr\n\nif ( 0 == 1 ) :\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True) # starts at 5.0e-5, drops to 1.0e-5 by epoch 30\n    lr = [lrfn(n) for n in range(30)]\n    plt.plot(lr)\nelse :\n    # Learning Rate Scheduler for fine-tuning jobs (first increase lr, then decrease)\n    print( 'using: LRSchedulers.FineTuningLR ')\n    lr_callback = LRSchedulers.FineTuningLR(\n        lr_start=1e-5\n        , lr_max=5e-5 * strategy.num_replicas_in_sync\n        , lr_min=1e-5\n        , lr_rampup_epochs=5\n        , lr_sustain_epochs=0\n        , lr_exp_decay=0.8\n        , verbose=1\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:56:58.834208Z","iopub.execute_input":"2022-12-05T20:56:58.835144Z","iopub.status.idle":"2022-12-05T20:56:58.850972Z","shell.execute_reply.started":"2022-12-05T20:56:58.835095Z","shell.execute_reply":"2022-12-05T20:56:58.849956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define the Model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications import DenseNet201\nimport efficientnet.tfkeras as efficientnet\n\n#define model\ndef create_model( input_shape, N_CLASSES, pretrained_model_trainable = False ):\n    if ( 0 == 1 ) :\n        #pretrained_model = tf.keras.applications.VGG16\n        #pretrained_model = tf.keras.applications.DenseNet201\n        #pretrained_model = tf.keras.applications.InceptionResNetV2\n        #pretrained_model = tf.keras.applications.InceptionV3\n        #pretrained_model = tf.keras.applications.MobileNet\n        #pretrained_model = tf.keras.applications.MobileNetV2\n        #pretrained_model = tf.keras.applications.NASNetMobile\n        #pretrained_model = tf.keras.applications.ResNet50\n        #pretrained_model = tf.keras.applications.ResNet101V2\n        #pretrained_model = tf.keras.applications.VGG19\n        #pretrained_model = tf.keras.applications.Xception\n        #pretrained_model = tf.keras.applications.DenseNet201 \n        #pretrained_model = EfficientNetB7\n        \n        pretrained_model = tf.keras.applications.xception.Xception(\n            include_top = False,\n            weights = 'imagenet',\n            input_shape = (*IMAGE_SHAPE,3)\n        )\n        pretrained_model.trainable = pretrained_model_trainable\n        model = tf.keras.Sequential([\n            pretrained_model,\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dense( 128, activation = 'relu' ),\n            tf.keras.layers.Dense( len( CLASSES ), activation = 'softmax' )\n        ])\n    elif ( 1 == 0 ) :\n        base_model = efn.EfficientNetB6(\n            weights='/kaggle/input/efficientnet/efficientnet-b6_noisy-student_notop.h5'\n            , include_top=False\n            , input_shape=input_shape\n        )\n\n        base_model.trainable = pretrained_model_trainable # Freeze layers\n        model = tf.keras.Sequential([\n            base_model,\n            L.GlobalAveragePooling2D(),\n            L.Dense( len( CLASSES ), activation='softmax')\n        ])\n    elif ( 0 == 1 ) :  # this model is slower ( but possibly more accurate )\n        # https://www.kaggle.com/code/chankhavu/a-beginner-s-tpu-kernel-single-model-0-97\n        feature_extractor= efficientnet.EfficientNetB7(\n            weights='noisy-student'\n            , include_top=False\n            , input_shape=[*IMAGE_SHAPE, 3])\n        feature_extractor.trainable = pretrained_model_trainable\n        model = tf.keras.Sequential([\n            feature_extractor,\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dense( len( CLASSES ), activation = 'softmax', dtype = 'float32' )\n        ], name = 'custom_EfficientNetB7_model')\n    else :\n        # DenseNet-201 is a convolutional neural network that is 201 layers deep.\n        #  You can load a pretrained version of the network trained on more than a million images from the ImageNet database.\n        #  The pretrained network can classify images into 1000 object categories, such as keyboard, mouse, pencil, and many animals. \n        base_model = DenseNet201(\n            include_top = False,   # whether to include the fully-connected layer at the top of the network.\n            weights = 'imagenet',\n            input_shape = ( IMAGE_SHAPE[0], IMAGE_SHAPE[1], 3),  # ? causes input images to be resized for DenseNet\n            #pooling = None  ? 'avg'\n            #classes = 1000\n        )\n        #print( base_model.summary() )\n        # trainable rnet\n        base_model.trainable = pretrained_model_trainable # True\n        dropout_rate = 0.2\n        model = tf.keras.Sequential([\n            base_model,\n            tf.keras.layers.GlobalAveragePooling2D(),\n            tf.keras.layers.Dropout( rate = dropout_rate ),\n            tf.keras.layers.Dense( units = len(CLASSES), activation='softmax', dtype='float32', kernel_regularizer = tf.keras.regularizers.l2() )\n        ], name = 'custom_DenseNet201_model' )\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:56:58.852571Z","iopub.execute_input":"2022-12-05T20:56:58.853094Z","iopub.status.idle":"2022-12-05T20:56:59.198951Z","shell.execute_reply.started":"2022-12-05T20:56:58.853041Z","shell.execute_reply":"2022-12-05T20:56:59.197957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Warm-up - Compile-Time set optimizer, loss and metrics","metadata":{}},{"cell_type":"code","source":"%%time\n#from sklearn.metrics import f1_score, precision_score, recall_score\n\n# ?? warmup top layers first before sub-model?\nWARMUP_LEARNING_RATE = 1e-4 * strategy.num_replicas_in_sync  #REPLICAS\n\nwith strategy.scope():  # mirrored_strategy.scope():\n    model = create_model( (None, None, 3), len(CLASSES) )\n    myMetric1 = F1Score( num_classes = len(CLASSES), average = 'macro', name = 'F1S_macro' ) # True positivies, false positives and false negatives are computed for each class and their unweighted mean is returned.\n    myMetric2 = F1Score( num_classes = len(CLASSES), average = 'micro', name = 'F1S_micro' ) # micro: True positivies, false positives and false negatives are computed globally.\n    \n    metric_list = [   # additional metrics beyond loss_function\n        tf.keras.metrics.SparseCategoricalAccuracy( name = 'SCA' )\n        #'sparse_categorical_accuracy'\n        #, tf.keras.metrics.SparseCategoricalCrossentropy( name = 'SCC', from_logits = False )\n        #, tf.keras.metrics.SparseCategoricalCrossentropy( name = 'SCCL', from_logits = True )\n        #'sparse_categorical_accuracy'\n        #, myMetric1\n        #, myMetric2    \n    ]\n\n    #loss_function = tf.keras.losses.SparseCategoricalCrossentropy( name = 'SCC' )   # How often predictions match integer labels ? 1.0 is perfect\n        # f1_loss\n\n    optimizer = tf.keras.optimizers.Adam( lr = WARMUP_LEARNING_RATE )\n\n    # ERR - ValueError: Metric (<tensorflow.python.keras.metrics.SparseCategoricalAccuracy object at 0x7fd4f78667d0>) passed to model.compile was created inside of a different distribution strategy scope than the model.\n    # All metrics must be created in the same distribution strategy scope as the model (in this case <tensorflow.python.distribute.tpu_strategy.TPUStrategy object at 0x7fd5ac927b50>).\n    # If you pass in a string identifier for a metric to compile the metric will automatically be created in the correct distribution strategy scope.\n    model.compile(\n        loss = 'sparse_categorical_crossentropy'  # loss_function\n        , optimizer = optimizer\n        , metrics = metric_list\n    )\n\nmodel.summary()  # should show model name\n# custom_EfficientNetB7_model - 64.3M Total params, 266K trainable ( 2560 features post base model )","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:56:59.200454Z","iopub.execute_input":"2022-12-05T20:56:59.201113Z","iopub.status.idle":"2022-12-05T20:57:41.673882Z","shell.execute_reply.started":"2022-12-05T20:56:59.201037Z","shell.execute_reply":"2022-12-05T20:57:41.672876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ! Data is ready\n\n# data dump of some samples\nprint(\"Training data shapes (3):\")\nfor ix, (image, label) in enumerate( get_train_dataset().take(3) ):\n    print(ix, image[ix].numpy().shape, label[ix].numpy().shape)\nprint(\"Training data label examples:\", len(label), label.numpy())\n\nprint(\"Validation data shapes (3):\")\nfor image, label in get_valid_dataset().take(3):\n    print(image.numpy().shape, label.numpy().shape)\nprint(\"Validation data label examples:\", len(label), label.numpy())\n\nprint(\"Test data shapes (3):\")\nfor image, idnum in get_test_dataset().take(3):\n    print(image.numpy().shape, idnum.numpy().shape)\nprint(\"Test data IDs:\", len(idnum), idnum.numpy().astype('U')) # U=unicode string","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:57:41.67521Z","iopub.execute_input":"2022-12-05T20:57:41.675489Z","iopub.status.idle":"2022-12-05T20:57:59.614144Z","shell.execute_reply.started":"2022-12-05T20:57:41.675451Z","shell.execute_reply":"2022-12-05T20:57:59.608198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Warm-up train-fit() based on learning-reate & validate ( subject to early-stopping )","metadata":{}},{"cell_type":"code","source":"%%time\nprint(\".fit() the model ( warm-up mode )\")\n# probably this should run ( quickly ) until it stops making significant progress\nSTEPS_PER_EPOCH = NUM_TRAIN_IMG // BATCH_SIZE\n\nfit_callbacks = [\n    tf.keras.callbacks.EarlyStopping(\n        monitor = 'val_loss'\n        , mode = 'min'\n        , min_delta = 0.05 # perhaps too high\n        , patience = 3 # this model is not very bouncy so we don't need much patience\n        , verbose = 1\n        , restore_best_weights = True\n    )\n    , myCallbackClass2( validation = ds_valid, current_fold = 0 )\n]\n\n#distributed_ds_train = strategy.experimental_distribute_dataset( ds_train )\n#distributed_ds_valid = strategy.experimental_distribute_dataset( ds_valid )\n\nwarmup_history = model.fit(\n    x = ds_train  # distributed_ds_train # \n    , steps_per_epoch = STEPS_PER_EPOCH  # = ~797 = 12753 training images / BATCH_SIZE = 16 * replicas\n    , validation_data = ds_valid  # distributed_ds_valid #   # ? dataset so probablly connect access [0] & [1]\n    , epochs = WARMUP_EPOCHS\n    , callbacks = fit_callbacks\n    , verbose = 2\n).history\n\n# warm-up results : 3 Epochs in 1m18s is quite fast, extend to more Epochs while we're efficiently improving\n# Epoch 1/3\n# 99/99 - 55s - loss: 3.0544 - sparse_categorical_accuracy: 0.3157 - val_loss: 2.0673 - val_sparse_categorical_accuracy: 0.5533\n# Epoch 2/3\n# 99/99 - 11s - loss: 1.7140 - sparse_categorical_accuracy: 0.6052 - val_loss: 1.4585 - val_sparse_categorical_accuracy: 0.6724\n# Epoch 3/3\n# 99/99 - 10s - loss: 1.2852 - sparse_categorical_accuracy: 0.7015 - val_loss: 1.2005 - val_sparse_categorical_accuracy: 0.7276\n# CPU times: user 22.5 s, sys: 1.11 s, total: 23.6 s\n# Wall time: 1min 18s -> 23s / Epoch ( first Epoch takes the most time, rest take only 10s ), 10 Epochs only takes 3m so 18s/E\n# with 512, takes 50s/iteration ( 5x longer )\n\n# w/ 15/20 Epoch can get to:  loss: 0.6927 - SCA: 0.8070 - SCC: 0.6927 - SCCL: 0.6927 - val_loss: 0.7296 - val_SCA: 0.8163 - val_SCC: 0.7296 - val_SCCL: 0.7296\n# Wall time: 18m 53s - 20 EPOCH - Accelerator = GPU P100 - 113s EPOCH 1, 54s for EPOCH 2+  (? 797 steps  = 12753 training images / batch_size = 16)\n# 797/797 - 53s - loss: 0.8170 - SCA: 0.7914 - val_loss: 0.8172 - val_SCA: 0.8006\n\n# Wall time: 3min 21s 99/99 - 10s - loss: 0.7145 - SCA: 0.8065 - val_loss: 0.7486 - val_SCA: 0.8144\n#  validation: count: 3712 — val_f1: 0.4145 — val_precision: 0.5343, — val_recall: 0.3944\n\n# with more training data, model doesn't do well\n# Epoch 5/20 - 531/531 - 249s - loss: 2.2327 - SCA: 0.6109 - val_loss: 3.4398 - val_SCA: 0.2249\n# validation: count: 3712 — val_f1: 0.1304 — val_precision: 0.3227, — val_recall: 0.1173","metadata":{"execution":{"iopub.status.busy":"2022-12-05T20:57:59.62371Z","iopub.execute_input":"2022-12-05T20:57:59.624788Z","iopub.status.idle":"2022-12-05T21:09:11.856263Z","shell.execute_reply.started":"2022-12-05T20:57:59.624536Z","shell.execute_reply":"2022-12-05T21:09:11.855319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# at this point, can we evaluate the model?\n# modelLoss, modelAccuracy = model.evaluate(\n#     ds_train # ds_train is infinite\n# )\n# print( f'Training Data - model Loss = {modelLoss}, Accuracy = {modelAccuracy}' )\nmodelLoss, modelAccuracy = model.evaluate(\n    ds_valid\n)\nprint( f'Validation Data - model Loss = {modelLoss}, Accuracy = {modelAccuracy}' )\n# Validation Data - model Loss = 1.7844997644424438, Accuracy = 0.7211745381355286\n# Validation Data - model Loss = 1.6651184558868408, Accuracy = 0.7599676847457886\n# Validation Data - model Loss = 2.4005961418151855, Accuracy = 0.6427801847457886 ( seems to be like val_SCA )","metadata":{"execution":{"iopub.status.busy":"2022-12-05T21:09:11.858002Z","iopub.execute_input":"2022-12-05T21:09:11.858321Z","iopub.status.idle":"2022-12-05T21:09:16.963385Z","shell.execute_reply.started":"2022-12-05T21:09:11.858286Z","shell.execute_reply":"2022-12-05T21:09:16.962149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model - Post-Warm-up - make all layers trainable","metadata":{}},{"cell_type":"code","source":"with strategy.scope():  # mirrored_strategy.scope():\n\n    for layer in model.layers:\n        layer.trainable = True # Unfreeze layers\n\n    myNadam = tf.keras.optimizers.Nadam(\n        learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Nadam\"\n    )\n\n    #loss_function = tf.keras.metrics.SparseCategoricalAccuracy( name='SCA', dtype=None) # 'sparse_categorical_accuracy'\n    loss_function = tf.keras.losses.SparseCategoricalCrossentropy( name = 'SCA', from_logits = True, reduction = tf.keras.losses.Reduction.SUM ) # 'sparse_categorical_accuracy'\n\n    compile_metrics = [\n        tf.keras.metrics.SparseCategoricalAccuracy( name = 'SCA' )  #'sparse_categorical_accuracy' # loss_function\n        , tf.keras.metrics.CategoricalAccuracy( name = 'CA' )  # 'categorical_accuracy'\n        , f1 \n        , custom_f1\n    ]\n\n    model.compile(\n        optimizer = myNadam,   # 'adam', 'nadam'\n        loss = 'sparse_categorical_crossentropy',   # f1_loss,\n        metrics = compile_metrics\n    )\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-12-05T21:09:16.965324Z","iopub.execute_input":"2022-12-05T21:09:16.965655Z","iopub.status.idle":"2022-12-05T21:09:17.15646Z","shell.execute_reply.started":"2022-12-05T21:09:16.965612Z","shell.execute_reply":"2022-12-05T21:09:17.155491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# base model \n# Total params: 21,137,168\n# Trainable params: 21,082,640\n# Non-trainable params: 54,528\n\n# efficientnet-b7 may have 64M total parameters\n# densenet201 has 18.5M Total params ( w/ no trainable becomes 199,784 trainable )","metadata":{"execution":{"iopub.status.busy":"2022-12-05T21:09:17.157775Z","iopub.execute_input":"2022-12-05T21:09:17.158036Z","iopub.status.idle":"2022-12-05T21:09:17.162979Z","shell.execute_reply.started":"2022-12-05T21:09:17.157993Z","shell.execute_reply":"2022-12-05T21:09:17.161609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor = 'val_loss',\n    min_delta = 0.001, # minimium amount of change to count as an improvement\n    patience = 5, # how many epochs to wait before stopping\n    restore_best_weights = True,\n    verbose = 1\n)\n\n# in general, this model mostly gets better each iteration so this is a bit of a waste plus the model is large/heavy\nmodelcheckpoint = tf.keras.callbacks.ModelCheckpoint( 'best_model.hdf5', monitor = 'sparse_categorical_accuracy', verbose = 1, save_best_only = True, mode = 'max' )","metadata":{"execution":{"iopub.status.busy":"2022-12-05T21:09:17.164477Z","iopub.execute_input":"2022-12-05T21:09:17.164758Z","iopub.status.idle":"2022-12-05T21:09:17.17409Z","shell.execute_reply.started":"2022-12-05T21:09:17.164727Z","shell.execute_reply":"2022-12-05T21:09:17.173234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# .fit() the model","metadata":{}},{"cell_type":"code","source":"%%time\n# training\n# add time-tracking in here\nSTEPS_PER_EPOCH = NUM_TRAIN_IMG // BATCH_SIZE\nEPOCHS = TRAIN_EPOCHS\n\nfit_callbacks = [ lr_callback, early_stopping, myCallbackClass2( validation = ds_valid, current_fold = 0 ) ]\nif CFG.wandb :\n    wandb.init( project = 'flower-classification-tpu-public', job_type = 'train', reinit = True )\n    # WandbCallback will automatically log history data from any metrics collected by keras: loss and anything passed into keras_model.compile().\n    #fit_callbacks = fit_callbacks + WandbCallback\n        # Initialize W&B run for experiment tracking\n    #wandb_run = wandb.init( entity='tnadeau', project='2022-11-Blend', job_type='train', name=f'fold_{fold}', reinit = True )\n    #assert wandb.run is not None\n    wandb_callback = WandbCallback() # save_model = False, log_weights = True\n    fit_callbacks = [ lr_callback, early_stopping, wandb_callback ]  # save_model = True log_gradients = True # training_data argument is required for gradient logging.\n    #fit_callbacks = [ early_stopping, WandbCallback( save_model = False, log_weights = True ) ]  # save_model = True log_gradients = True # training_data argument is required for gradient logging.\n\n#fit_callbacks = [ myCallbackClass2(npt_exp, validation=(x_val, y_val), current_fold=k_fold)]\n\n# better to move this to k-fold technique\nhistory = model.fit(\n    ds_train,\n    validation_data = ds_valid,\n    epochs = EPOCHS,\n    steps_per_epoch = STEPS_PER_EPOCH,\n    callbacks = fit_callbacks,\n    class_weight = weight_per_class,  # Ted - Added\n    verbose = 2\n)\n\nif CFG.wandb :\n    wandb.finish()\n\n#\n# Epoch 1/15 Epoch 00001: LearningRateScheduler reducing learning rate to 5e-05.\n# 99/99 [==============================] - 109s 693ms/step - loss: 4.2723 - sparse_categorical_accuracy: 0.1285 - val_loss: 2.8269 - val_sparse_categorical_accuracy: 0.3817\n# Epoch 00015: LearningRateScheduler reducing learning rate to 1.9264677851328125e-05.\n# 99/99 [==============================] - 48s 491ms/step - loss: 0.0549 - sparse_categorical_accuracy: 0.9953 - val_loss: 0.3277 - val_sparse_categorical_accuracy: 0.9221\n\n# with accelerator = None ( CPU ), then 400% ( ? slow ), GPU T4 x 2 ( shows 1 GPU at 50% )\n# with 512, Epoch = 50s","metadata":{"execution":{"iopub.status.busy":"2022-12-05T21:09:17.175634Z","iopub.execute_input":"2022-12-05T21:09:17.175949Z","iopub.status.idle":"2022-12-05T21:41:41.306921Z","shell.execute_reply.started":"2022-12-05T21:09:17.175911Z","shell.execute_reply":"2022-12-05T21:41:41.30576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n* 99/99 - 172s - loss: 4.6632 - sparse_categorical_accuracy: 0.0244 - categorical_accuracy: 0.0042 - f1: 0.0000e+00 - custom_f1: 78.5914 - val_loss: 4.4771 - val_sparse_categorical_accuracy: 0.0415 - val_categorical_accuracy: 0.0057 - val_f1: 0.0000e+00 - val_custom_f1: 79.2682\n* \n* Epoch 00026: LearningRateScheduler reducing learning rate to 1.4496393867966709e-05.\n* 99/99 - 13s - loss: 0.0116 - sparse_categorical_accuracy: 0.9953 - categorical_accuracy: 0.0214 - f1: 0.0191 - custom_f1: 1.2919 - val_loss: 0.3025 - val_sparse_categorical_accuracy: 0.9283 - val_categorical_accuracy: 0.0213 - val_f1: 0.0187 - val_custom_f1: 1.6426\n* Restoring model weights from the end of the best epoch.\n* Epoch 00026: early stopping\n\n* Epoch 00022: LearningRateScheduler reducing learning rate to 2.0977524091715595e-05.\n* 99/99 - 52s - loss: 0.0088 - sparse_categorical_accuracy: 0.9970 - categorical_accuracy: 0.0214 - f1: 0.0191 - custom_f1: 1.2400 - val_loss: 0.2305 - val_sparse_categorical_accuracy: 0.9531 - val_categorical_accuracy: 0.0210 - val_f1: 0.0189 - val_custom_f1: 1.3983\n* Restoring model weights from the end of the best epoch.\n* Epoch 00022: early stopping\n* CPU times: user 4min 3s, sys: 24.4 s, total: 4min 27s\n* Wall time: 24min 1s","metadata":{}},{"cell_type":"code","source":"# at this point, can we evaluate the model?\n# modelLoss, modelAccuracy = model.evaluate(\n#     ds_train # ds_train is infinite\n# )\n# print( f'Training Data - model Loss = {modelLoss}, Accuracy = {modelAccuracy}' )\nmodelLoss, modelAccuracy, CA, f1, custom_f1 = model.evaluate(  # too many values to unpack (expected 2)\n    ds_valid\n)\nprint( f'Validation Data - model Loss = {modelLoss}, Accuracy = {modelAccuracy}' )\n#Validation Data - model Loss = 0.9083672761917114, Accuracy = 0.8809267282485962\n# 29/29 [==============================] - 5s 129ms/step - loss: 0.4066 - SCA: 0.9526 - CA: 0.0207 - f1: 0.0186 - custom_f1: 1.6619\n# Validation Data - model Loss = 0.4065918028354645, Accuracy = 0.9525861740112305\n# 29/29 [==============================] - 4s 127ms/step - loss: 0.4216 - SCA: 0.9502 - CA: 0.0210 - f1: 0.0187 - custom_f1: 1.6094\n# Validation Data - model Loss = 0.4216325581073761, Accuracy = 0.9501616358757019","metadata":{"execution":{"iopub.status.busy":"2022-12-05T21:41:41.308733Z","iopub.execute_input":"2022-12-05T21:41:41.309027Z","iopub.status.idle":"2022-12-05T21:41:46.895357Z","shell.execute_reply.started":"2022-12-05T21:41:41.308985Z","shell.execute_reply":"2022-12-05T21:41:46.894027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# should be improved to be agnostic to history keys\n# should show trendlines, formulas and projection ( as f( EPOCH, time ) )\ndef plot_hist( history, EPOCHS ):\n    \n    epochs_ct = len( history.history['loss'] )\n    plt.subplot(2,1,1)\n    loss = history.history['loss']\n    vloss = history.history['val_loss']\n    plt.plot(range(1,epochs_ct+1),loss,c='b',label='loss')\n    plt.plot(range(1,epochs_ct+1),vloss,c='r',label='val_loss')\n    plt.legend()\n    \n    plt.subplot(2,1,2)\n    acc = history.history['SCA']\n    vacc = history.history['val_SCA']\n    plt.plot(range(1,epochs_ct+1),acc,c='b',label='SCA')\n    plt.plot(range(1,epochs_ct+1),vacc,c='r',label='val_SCA')\n    plt.legend()\n    \n    plt.plot()","metadata":{"execution":{"iopub.status.busy":"2022-12-05T21:41:46.897131Z","iopub.execute_input":"2022-12-05T21:41:46.898235Z","iopub.status.idle":"2022-12-05T21:41:46.907941Z","shell.execute_reply.started":"2022-12-05T21:41:46.898186Z","shell.execute_reply":"2022-12-05T21:41:46.906721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_hist( history, EPOCHS )\n# use curve-fit to predict asymptope ","metadata":{"execution":{"iopub.status.busy":"2022-12-05T21:41:46.909342Z","iopub.execute_input":"2022-12-05T21:41:46.909625Z","iopub.status.idle":"2022-12-05T21:41:47.311356Z","shell.execute_reply.started":"2022-12-05T21:41:46.909587Z","shell.execute_reply":"2022-12-05T21:41:47.310608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the model\nmodel.save('Petals_to_the_Metal_save1.h5')","metadata":{"execution":{"iopub.status.busy":"2022-12-05T21:41:47.312535Z","iopub.execute_input":"2022-12-05T21:41:47.312786Z","iopub.status.idle":"2022-12-05T21:41:56.380591Z","shell.execute_reply.started":"2022-12-05T21:41:47.312749Z","shell.execute_reply":"2022-12-05T21:41:56.37944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation - Validation on known ( train & valid )","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.metrics import f1_score, precision_score, recall_score\n\ndataset = get_train_dataset()\n\ndataset = dataset.unbatch().batch( 9*9 )\nbatch = iter(dataset)\n\nimages, labels = next(batch)\nprobabilities = model.predict(images)\npredictions = np.argmax( probabilities, axis = -1 )\nprint( 'predicted labels', predictions.shape, predictions )\nprint( 'correct labels', labels.shape, labels )\n\n# perhaps change to 'micro'\naverage_method = 'micro'  # or 'macros'\nscore = sklearn.metrics.f1_score( labels, predictions, labels=range(len(CLASSES)), average=average_method, zero_division = 'warn' )\nprecision = sklearn.metrics.precision_score( labels, predictions, labels=range(len(CLASSES)), average=average_method, zero_division = 'warn')\nrecall = sklearn.metrics.recall_score( labels, predictions, labels=range(len(CLASSES)), average=average_method, zero_division = 'warn')\n\n#cmat = confusion_matrix(labels, predictions, labels=range(len(CLASSES)))\n#display_confusion_matrix(cmat, score, precision, recall)\nprint('f1 score: {:.3f}, precision: {:.3f}, recall: {:.3f}'.format(score, precision, recall)); print()\n# f1 score: 0.113, precision: 0.112, recall: 0.115\n#f1 score: 0.356, precision: 0.371, recall: 0.368\n#f1 score: 0.471, precision: 0.471, recall: 0.471 - tuned\n\ndisplay_batch_images((images, labels), predictions)\n# ( shows true [, prediction])","metadata":{"execution":{"iopub.status.busy":"2022-12-05T21:41:56.384592Z","iopub.execute_input":"2022-12-05T21:41:56.384902Z","iopub.status.idle":"2022-12-05T21:42:56.88143Z","shell.execute_reply.started":"2022-12-05T21:41:56.38487Z","shell.execute_reply":"2022-12-05T21:42:56.880244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# classification_report(s)\n* Some flowers are 'most' confused\n * ? wrong prediction - to: wild geranium, common dandelion, iris, thorn apple, wild rose, ","metadata":{}},{"cell_type":"code","source":"%%time\n# run the model on known training set\nfrom sklearn.metrics import classification_report\n\nx_train = get_train_dataset_preview( ordered = True )\ny_train = next(iter(x_train.unbatch().map(lambda image, label: label).batch(NUM_TRAIN_IMG))).numpy()\n\n#train_dataset = get_training_data_preview( do_aug = False, do_repeat = False, do_shuffle = False ) \n#batch = iter(train_dataset)\n#x_train, y_train = next(batch)\n#x_train = train_dataset.map( lambda image, label: image )  # train is generically an 'infinite' dataset\n#x_train, y_train = next( iter( train_dataset.unbatch().map(lambda image, label: label).batch(NUM_TRAIN_IMG) ) )\n#x_train, y_train = next( iter( train_dataset.batch(NUM_TRAIN_IMG) ) )\n#x_train = train_dataset.map( lambda image, label: image)\nprint( len( y_train ) ) # 128  should be 12753\n\ntrain_predict = model.predict( x_train )\ntrain_preds = np.argmax( train_predict, axis=-1)\n\naverage_method = 'macro' # macro, weighted, samples\ntrain_precision_score, train_recall_score, train_fbeta_score, train_support_score = sklearn.metrics.precision_recall_fscore_support( y_true = y_train, y_pred = train_preds, average = average_method )\nprint( 'train: precision_recall_fscore_support', train_precision_score, train_recall_score, train_fbeta_score, train_support_score )\n\ntrain_f1_score = sklearn.metrics.f1_score( y_train, train_preds, average = average_method )\ntrain_precision_score = sklearn.metrics.precision_score( y_train, train_preds, average = average_method )\ntrain_recall_score = sklearn.metrics.recall_score( y_train, train_preds, average = average_method )\ntrain_accuracy_score = sklearn.metrics.accuracy_score( y_train, train_preds )\nprint( 'train: f1, precision, recall', train_f1_score, train_precision_score, train_recall_score, train_accuracy_score )\n\nprint( classification_report( y_true = y_train, y_pred = train_preds, target_names = CLASSES, digits = 3 ) )  # these should all be 1.00 ideally ( esp. on known training data )","metadata":{"execution":{"iopub.status.busy":"2022-12-05T21:42:56.883026Z","iopub.execute_input":"2022-12-05T21:42:56.883301Z","iopub.status.idle":"2022-12-05T21:44:41.363537Z","shell.execute_reply.started":"2022-12-05T21:42:56.883271Z","shell.execute_reply":"2022-12-05T21:44:41.362383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_valid = ds_valid.map( lambda image, label: image )\nvalid_predict = model.predict( x_valid )\nvalid_preds = np.argmax( valid_predict, axis = -1 )\n\naverage_method = 'macro' # micro macro, weighted, samples\nvalid_f1_score = sklearn.metrics.f1_score( y_valid, valid_preds, average = average_method )\nvalid_precision_score = sklearn.metrics.precision_score( y_valid, valid_preds, average = average_method )\nvalid_recall_score = sklearn.metrics.recall_score( y_valid, valid_preds, average = average_method )\nvalid_accuracy_score = sklearn.metrics.accuracy_score( y_valid, valid_preds )\nprint( 'valid: f1, precision, recall, accuracy', valid_f1_score, valid_precision_score, valid_recall_score, valid_accuracy_score )\n\noutput_dict = classification_report( y_valid, valid_preds, target_names = CLASSES, output_dict = True )\nfig, ax = plt.subplots(figsize=(6,22)) \nsns.heatmap(pd.DataFrame(output_dict).iloc[:-1, :].T, annot=True, fmt='.3f', cmap = 'viridis', ax=ax)\nprint( classification_report( y_valid, valid_preds, target_names = CLASSES, digits = 3 ) )","metadata":{"execution":{"iopub.status.busy":"2022-12-05T21:44:41.365629Z","iopub.execute_input":"2022-12-05T21:44:41.366Z","iopub.status.idle":"2022-12-05T21:44:49.785684Z","shell.execute_reply.started":"2022-12-05T21:44:41.365952Z","shell.execute_reply":"2022-12-05T21:44:49.784491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndataset = get_valid_dataset()\n\ndataset = dataset.unbatch().batch(8*8)  # this only gets 20, we want them all, without it, we only get 16\nbatch = iter(dataset)\n\nimages, labels = next( batch )\n\nprobabilities = model.predict( images )\npredictions = np.argmax( probabilities, axis = -1 )\nprint( 'predicted labels', predictions.shape, predictions )\nprint( 'correct labels', labels.shape, labels )\ndisplay_batch_images((images, labels), predictions)\n\n\nif ( 0 == 1 ) :\n    dataset = dataset.unbatch().batch( NUM_VALID_IMG )  # this only gets 20, we want them all, without it, we only get 16\n    # 2022-12-01 20:01:47.243723: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1642070016 exceeds 10% of free system memory.\n    batch = iter(dataset)\n    images, labels = next( batch )\n\nprobabilities = model.predict( images )\npredictions = np.argmax( probabilities, axis = -1 )\n\naverage_method = 'micro'  # or 'macros'\nscore = f1_score( labels, predictions, labels=range(len(CLASSES)), average=average_method, zero_division = 'warn' )\nprecision = precision_score( labels, predictions, labels=range(len(CLASSES)), average=average_method, zero_division = 'warn')\nrecall = recall_score( labels, predictions, labels=range(len(CLASSES)), average=average_method, zero_division = 'warn')\n\n#cmat = confusion_matrix(labels, predictions, labels=range(len(CLASSES)))\n#display_confusion_matrix(cmat, score, precision, recall)\nprint('validation count = {:d} f1 score: {:.3f}, precision: {:.3f}, recall: {:.3f}'.format(len(labels), score, precision, recall)); print()\n# ? f1 score: 0.948, precision: 0.952, recall: 0.945\n# f1 score: 0.113, precision: 0.112, recall: 0.115\n# 3712 - f1 score: 0.687, precision: 0.672, recall: 0.746\n# validation count = 100 f1 score: 0.336, precision: 0.354, recall: 0.352\n# validation count = 100 f1 score: 0.432, precision: 0.436, recall: 0.432\n# validation count = 100 f1 score: 0.950, precision: 0.950, recall: 0.950\n\n# we should show:\n#  images that don't work\n#  confidence levels\n# 1 black-eye Susan was mis-categorized as sunflower","metadata":{"execution":{"iopub.status.busy":"2022-12-05T21:44:49.787384Z","iopub.execute_input":"2022-12-05T21:44:49.787711Z","iopub.status.idle":"2022-12-05T21:45:30.803967Z","shell.execute_reply.started":"2022-12-05T21:44:49.787669Z","shell.execute_reply":"2022-12-05T21:45:30.802956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mis-matches on validation data\n# https://www.kaggle.com/code/georgezoto/computer-vision-petals-to-the-metal#Step-3:-Loading-the-Competition-Data\nmismatches = sum( predictions != labels.numpy() )  # labels is a tensor, predictions is a list\nNUM_VALIDATION_IMAGES = len( predictions )\nprint('Number of mismatches (mis-categorization) on validation data: {} out of {} or ({:.2%})'.format(mismatches, NUM_VALIDATION_IMAGES, mismatches/NUM_VALIDATION_IMAGES))","metadata":{"execution":{"iopub.status.busy":"2022-12-05T21:45:30.805333Z","iopub.execute_input":"2022-12-05T21:45:30.805612Z","iopub.status.idle":"2022-12-05T21:45:30.812441Z","shell.execute_reply.started":"2022-12-05T21:45:30.805581Z","shell.execute_reply":"2022-12-05T21:45:30.811206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom sklearn.metrics import confusion_matrix  # (y_true, y_pred, *, labels=None, sample_weight=None, normalize=None)[source]¶\nfrom sklearn.metrics import ConfusionMatrixDisplay # (confusion_matrix, *, display_labels=None)[source]¶\n\ncmat = confusion_matrix(\n    labels.numpy(),\n    predictions,\n    labels = labels,\n)\ndisp = ConfusionMatrixDisplay( cmat )  # is large 104 x 104\nfig, ax = plt.subplots(figsize=(25,25))\ndisp.plot( ax = ax )\nplt.show()\n#display_confusion_matrix(cmat, score, precision, recall)","metadata":{"execution":{"iopub.status.busy":"2022-12-05T21:45:30.813942Z","iopub.execute_input":"2022-12-05T21:45:30.814308Z","iopub.status.idle":"2022-12-05T21:45:43.79428Z","shell.execute_reply.started":"2022-12-05T21:45:30.814268Z","shell.execute_reply":"2022-12-05T21:45:43.793502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# valid_acc_jnp, valid_preds_jnp, valid_true_jnp = model.evaluate(  \n#     ds_valid , \n#     #NUM_VALIDATION_IMAGES , \n#     BATCH_SIZE\n# )\n\n# val_acc = valid_acc_jnp.item()*100\n# print(f\"Validation Accuracy = {val_acc:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2022-12-05T21:45:43.795625Z","iopub.execute_input":"2022-12-05T21:45:43.796031Z","iopub.status.idle":"2022-12-05T21:45:43.800169Z","shell.execute_reply.started":"2022-12-05T21:45:43.795998Z","shell.execute_reply":"2022-12-05T21:45:43.799158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predication on unknown test, then submission","metadata":{}},{"cell_type":"code","source":"#load the test dataset\ntest_ds = get_test_dataset(ordered=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-05T21:45:43.80156Z","iopub.execute_input":"2022-12-05T21:45:43.801962Z","iopub.status.idle":"2022-12-05T21:45:43.842635Z","shell.execute_reply.started":"2022-12-05T21:45:43.801932Z","shell.execute_reply":"2022-12-05T21:45:43.841595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# seperate image and id \ntest_images_ds = test_ds.map(lambda image, idnum: image)\ntest_id_ds = test_ds.map(lambda image, idnum: idnum)","metadata":{"execution":{"iopub.status.busy":"2022-12-05T21:45:43.843987Z","iopub.execute_input":"2022-12-05T21:45:43.844931Z","iopub.status.idle":"2022-12-05T21:45:43.886653Z","shell.execute_reply.started":"2022-12-05T21:45:43.844891Z","shell.execute_reply":"2022-12-05T21:45:43.885491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# perform prediction on the test data using trained model\npred = model.predict( test_images_ds )  # pred is (7382, 104)\nprint( pred.shape, pred )# an array of 7382 arrays each with 104 values which are the ? relative probabilities )\n\n# since the model output is sparse we only need the max value index\n# would also like to know confidence\nlabel_pred = np.argmax( pred, axis = -1 )","metadata":{"execution":{"iopub.status.busy":"2022-12-05T21:45:43.887962Z","iopub.execute_input":"2022-12-05T21:45:43.888228Z","iopub.status.idle":"2022-12-05T21:46:35.754029Z","shell.execute_reply.started":"2022-12-05T21:45:43.888197Z","shell.execute_reply":"2022-12-05T21:46:35.752697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display some predictions on unknown/test\n# display some that we are quite sure of\n# display some that we are unsure of\n#display_batch_images((images, labels), pred)","metadata":{"execution":{"iopub.status.busy":"2022-12-05T21:46:35.755912Z","iopub.execute_input":"2022-12-05T21:46:35.756304Z","iopub.status.idle":"2022-12-05T21:46:35.761008Z","shell.execute_reply.started":"2022-12-05T21:46:35.756256Z","shell.execute_reply":"2022-12-05T21:46:35.759844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pred = pd.DataFrame( pred )\ndf_pred['max'] = df_pred.aggregate( ['max'], axis = 1)\ndf_pred['label'] = label_pred\ndisplay( df_pred )\ndisplay( df_pred.describe() ) # every col ( class ) has a max of near 1.0 ( which is the one that it is most likely )","metadata":{"execution":{"iopub.status.busy":"2022-12-05T21:46:35.76303Z","iopub.execute_input":"2022-12-05T21:46:35.763504Z","iopub.status.idle":"2022-12-05T21:46:41.47057Z","shell.execute_reply.started":"2022-12-05T21:46:35.763421Z","shell.execute_reply":"2022-12-05T21:46:41.469517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# some rows that we are pretty sure of\n#display( df_pred.iloc[[4,1502,1989,7378,7379,7380]] )  # 46 #58 70\n#display( df_pred[ df_pred['max'] > 0.999 ] ) # >0.999 3781 rows of 7382\n#display( df_pred[ df_pred['max'] > 0.99 ] ) # >0.99 5318 rows of 7382\n#display( df_pred[ df_pred['max'] > 0.95 ] ) # >0.95 6072 rows of 7382\n#display( df_pred[ df_pred['max'] > 0.80 ] ) # >0.80 6676 rows of 7382\n#display( df_pred[ df_pred['max'] > 0.70 ] ) # >0.70 6841 rows of 7382\ndisplay( df_pred[ df_pred['max'] > 0.50 ] ) # >0.70 7178 rows of 7382, 7230, 7166","metadata":{"execution":{"iopub.status.busy":"2022-12-05T21:46:41.472315Z","iopub.execute_input":"2022-12-05T21:46:41.472913Z","iopub.status.idle":"2022-12-05T21:46:41.61453Z","shell.execute_reply.started":"2022-12-05T21:46:41.472851Z","shell.execute_reply":"2022-12-05T21:46:41.613451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# some rows that are troubling ( confidence is low )\ndisplay( df_pred.iloc[[4,1502,1989,7378,7379,7380]] )  # 46 #58 70\n","metadata":{"execution":{"iopub.status.busy":"2022-12-05T21:46:41.616273Z","iopub.execute_input":"2022-12-05T21:46:41.616551Z","iopub.status.idle":"2022-12-05T21:46:41.707625Z","shell.execute_reply.started":"2022-12-05T21:46:41.616518Z","shell.execute_reply":"2022-12-05T21:46:41.706474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display( df_pred[ df_pred['max'] < 0.30 ] )","metadata":{"execution":{"iopub.status.busy":"2022-12-05T21:46:41.709105Z","iopub.execute_input":"2022-12-05T21:46:41.709459Z","iopub.status.idle":"2022-12-05T21:46:42.013148Z","shell.execute_reply.started":"2022-12-05T21:46:41.709413Z","shell.execute_reply":"2022-12-05T21:46:42.012354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Id dataset -> numpy \nids = next(iter(test_id_ds.unbatch().batch(NUM_TEST_IMG))).numpy().astype('U')\n\n# dictionary to create dataframe from data\nsubmission_dict = {\n    'id':ids,\n    'label':label_pred\n}\ndf_submission = pd.DataFrame(submission_dict)\ndisplay( df_submission.head(10) )\ndisplay( df_submission.tail(10) )\n\n# \tid\tlabel\n# 0\t252d840db\t67\n# 1\t1c4736dea\t28 (? wrong is 4)\n# 2\tc37a6f3e9\t83  # possibly 77 ?81, ? wrong is 103\n# 3\t00e4f514e\t103\n# 4\t59d1b6146\t46 #58 70 ( ? wrong is 67)\n# 5\t8d808a07b\t53\n# 6\taeb67eefb\t52 ( wrong is 103 )\n# 7\t53cfc6586\t29 48 ( ? wrong is 49 )\n# 8\taaa580243\t82 ( ? wrong is 67 )\n# 9\td907ca7c0\t13\n\n# id\tlabel\n# 7372\t298ade3a4\t49\n# 7373\t8361401fa\t45 47\n# 7374\t3522d5b4e\t47\n# 7375\tf65475a24\t48\n# 7376\tce3c158fa\t74\n# 7377\tc785abe6f\t7\n# 7378\t9b9c0e574\t68 81 103\n# 7379\te46998f4d\t53 103 48 86\n# 7380\t523df966b\t102 49\n# 7381\te86e2a592\t62","metadata":{"execution":{"iopub.status.busy":"2022-12-05T21:46:42.014291Z","iopub.execute_input":"2022-12-05T21:46:42.015225Z","iopub.status.idle":"2022-12-05T21:47:13.139118Z","shell.execute_reply.started":"2022-12-05T21:46:42.015179Z","shell.execute_reply":"2022-12-05T21:47:13.137836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_agg = np.asarray([[label, (y_train == index).sum()] for index, label in enumerate(CLASSES)])\nvalid_agg = np.asarray( [[label, ( y_valid == index ).sum() ] for index, label in enumerate(CLASSES)] )\ntest_agg = np.asarray( [[label, ( df_submission['label'] == index).sum() ] for index, label in enumerate(CLASSES)] )\n#print( test_agg )\n\nfig, (ax1, ax2) = plt.subplots( 2, 1, figsize=(24, 64) )\n\n# ax1 = sns.barplot(x=train_agg[...,1], y=train_agg[...,0], order=CLASSES, ax=ax1)\n# ax1.set_title('Train', fontsize=30)\n# ax1.tick_params(labelsize=16)\n\nx_vals = [ eval(i) for i in valid_agg[...,1] ]\nax1 = sns.barplot( x = x_vals, y = valid_agg[...,0], order = CLASSES, ax=ax1)\nax1.set_title('Validation', fontsize=30)\nax1.tick_params( labelsize = 16)\n\nx_vals = [ eval(i) for i in test_agg[...,1] ]\nax2 = sns.barplot( x = x_vals, y = test_agg[...,0], order = CLASSES, ax = ax2)\nax2.set_title( 'Test', fontsize = 30 )\nax2.tick_params( labelsize = 16 )\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-12-05T22:10:07.92631Z","iopub.execute_input":"2022-12-05T22:10:07.926679Z","iopub.status.idle":"2022-12-05T22:10:12.645308Z","shell.execute_reply.started":"2022-12-05T22:10:07.926644Z","shell.execute_reply":"2022-12-05T22:10:12.644201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save as csv\ndf_submission.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-05T21:47:13.140636Z","iopub.execute_input":"2022-12-05T21:47:13.141623Z","iopub.status.idle":"2022-12-05T21:47:13.16359Z","shell.execute_reply.started":"2022-12-05T21:47:13.141571Z","shell.execute_reply":"2022-12-05T21:47:13.162511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print( \"Local (NY) Time: \", dt.now( tz_NY ).strftime(\"%Y-%m-%d %H:%M:%S\") )","metadata":{"execution":{"iopub.status.busy":"2022-12-05T21:50:53.847267Z","iopub.execute_input":"2022-12-05T21:50:53.847639Z","iopub.status.idle":"2022-12-05T21:50:53.854476Z","shell.execute_reply.started":"2022-12-05T21:50:53.847605Z","shell.execute_reply":"2022-12-05T21:50:53.853372Z"},"trusted":true},"execution_count":null,"outputs":[]}]}